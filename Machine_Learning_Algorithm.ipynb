{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrdarJEKdgCpPHLmoGVjzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khorzhengyu/project518/blob/main/Machine_Learning_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Logistic Regression - Imbalanced data ##\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "log_reg_params = { 'C': [0.001, 0.01, 0.1, 1.0], 'max_iter' :[2000], 'random_state':[13]}\n",
        "\n",
        "# Create a cross-validation strategy (Stratified K-Fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "# Initialize GridSearchCV with Logistic Regression, hyperparameter grid, and cross-validation\n",
        "grid_log_reg = GridSearchCV(LogisticRegression(random_state = 13), log_reg_params, cv=cv, scoring = 'recall')\n",
        "\n",
        "# Fit the grid search to your training data\n",
        "grid_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters for Logistic Regression:\", grid_log_reg.best_params_['C'])\n",
        "\n",
        "# Get the best estimator\n",
        "best_log_reg = grid_log_reg.best_estimator_"
      ],
      "metadata": {
        "id": "X8XBILzsJdyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictionLR = grid_log_reg.predict(X_test)\n",
        "print(confusion_matrix(y_test,predictionLR))\n",
        "print(classification_report(y_test,predictionLR))"
      ],
      "metadata": {
        "id": "AyNCxKnwOWhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,predictionLR)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "WOtQn4hePtZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = best_log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pa4kFJ44QVVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    best_log_reg, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "vF5aGY2_YjqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Logistic Regression - Undersampling ##\n",
        "rus = RandomUnderSampler(random_state = 42)\n",
        "X_rus, y_rus = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "13Q_LI3kMTD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=False)"
      ],
      "metadata": {
        "id": "lJjjX-OpD0ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_under_pipeline = make_pipeline(RandomUnderSampler(random_state=42),\n",
        "                                        LogisticRegression(random_state=13, max_iter=1500))"
      ],
      "metadata": {
        "id": "8gn3yKEz42L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid, specifying 'logisticregression__' for the C parameter\n",
        "new_params = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1,1.0]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_under_logistic = GridSearchCV(\n",
        "    logistic_under_pipeline,  # Your pipeline with Logistic Regression\n",
        "    param_grid=new_params,  # The hyperparameter grid\n",
        "    cv= cv ,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_under_logistic.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "6xgDrxKX71uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_under_logistic.best_params_)\n",
        "print('Best score:', grid_under_logistic.best_score_)"
      ],
      "metadata": {
        "id": "aWb5XHnz_bTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_under_logistic.best_estimator_.named_steps['logisticregression'].predict(X_test)"
      ],
      "metadata": {
        "id": "5cjBR5eYAuMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "tjV2cVCIBVOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "-8_5P9Z9B7SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_under_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f0dcQQeV48FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_under_logistic, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "3nftdzuf5gq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Logistic Regression - Oversampling ##\n",
        "ros = RandomOverSampler(random_state = 42)\n",
        "X_ros, y_ros = ros.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "BEpF0WEYEJJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate the unique values and their counts\n",
        "unique_values, counts = np.unique(y_ros, return_counts=True)\n",
        "\n",
        "# Print the results\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Class {value}: {count} samples')\n"
      ],
      "metadata": {
        "id": "7bRZuNe6EeiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_over_pipeline = make_pipeline(RandomOverSampler(random_state=42),\n",
        "                                        LogisticRegression(random_state=13, max_iter=1500))\n",
        "\n",
        "score2 = cross_val_score(logistic_over_pipeline, X_train, y_train, scoring='recall', cv= cv)\n",
        "print(\"Cross Validation Recall Scores are: {}\".format(score2))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score2.mean()))"
      ],
      "metadata": {
        "id": "DlUCN2iyEuTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid, specifying 'logisticregression__' for the C parameter\n",
        "new_params = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1,1.0]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_over_logistic = GridSearchCV(\n",
        "    logistic_over_pipeline,  # Your pipeline with Logistic Regression\n",
        "    param_grid=new_params,  # The hyperparameter grid\n",
        "    cv= cv ,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_over_logistic.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "ZJZ_AjQtE6nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_over_logistic.best_params_)\n",
        "print('Best score:', grid_over_logistic.best_score_)"
      ],
      "metadata": {
        "id": "HJhdKOe2HvgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_over_logistic.best_estimator_.named_steps['logisticregression'].predict(X_test)"
      ],
      "metadata": {
        "id": "gZ-l6MIaFFfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "KNlc7JlFFFyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_over = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_over), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "7bmN1z4sFecd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_over_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I19dpShv48Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_over_logistic, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "S4DDZ6pq48iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Logistic Regression - SMOTE ##\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_pipeline = make_pipeline(SMOTE(random_state=42),\n",
        "                              LogisticRegression(max_iter = 2000, random_state=13))\n",
        "score2 = cross_val_score(smote_pipeline, X_train, y_train, cv= cv)\n",
        "print(\"Cross Validation Recall Scores are: {}\".format(score2))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score2.mean()))"
      ],
      "metadata": {
        "id": "-dVVfxImGKAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid, specifying 'logisticregression__' for the C parameter\n",
        "new_params = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1,1.0]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_smote_logistic = GridSearchCV(\n",
        "    smote_pipeline,  # Your pipeline with Logistic Regression\n",
        "    param_grid=new_params,  # The hyperparameter grid\n",
        "    cv= cv ,  # Your cross-validation strategy (StratifiedKFold)\n",
        "      # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_smote_logistic.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "fQ-j-h_tM4ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_smote_logistic.best_params_)\n",
        "print('Best score:', grid_smote_logistic.best_score_)"
      ],
      "metadata": {
        "id": "piFt1T-6M43W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_smote_logistic.best_estimator_.named_steps['logisticregression'].predict(X_test)"
      ],
      "metadata": {
        "id": "hWungTxtM5AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "3rfAy7vKM5Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_smote = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_smote), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "nHUY_sjWPJ9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_smote_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6ETnPUEqPKDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_smote_logistic, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "CptqfbYwPV9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vyLT6AJuGWXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest - imbalanced dataset #\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=False)"
      ],
      "metadata": {
        "id": "_UHfpYz-Zt_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=13)"
      ],
      "metadata": {
        "id": "6ds8kxjrq_EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cross_val_score(rf, X_train, y_train, cv=kf)\n",
        "print(\"Cross Validation Recall scores are: {}\".format(score))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score.mean()))"
      ],
      "metadata": {
        "id": "_DhCDJW9q_Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'random_state': [13]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(rf, param_grid=params, cv=kf,\n",
        "                          scoring='recall').fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7WXTb0mYq_bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_rf.best_params_)\n",
        "print('Best score:', grid_rf.best_score_)"
      ],
      "metadata": {
        "id": "EmrdtB9ZSycv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_rf.predict(X_test)"
      ],
      "metadata": {
        "id": "QLajdZCft-Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "JP1QRsGNt-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_matrix = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "PxXnVMuMzZG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AxuE0GbPz3-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "ggZ7UfRkzZP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest - Oversampling ##\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "rf_over_pipeline = make_pipeline(RandomOverSampler(random_state=42),\n",
        "                              RandomForestClassifier(n_estimators = 100, random_state=13))"
      ],
      "metadata": {
        "id": "1InSZVkfLxXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the hyperparameter grid for Random Forest\n",
        "params = {\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__random_state': [13]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_over_rf = GridSearchCV(\n",
        "    rf_over_pipeline,  # Your pipeline with Random Forest\n",
        "    param_grid=params,  # The hyperparameter grid\n",
        "    cv=cv,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_over_rf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "U3JIpteTMlHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_over_rf.best_params_)\n",
        "print('Best score:', grid_over_rf.best_score_)"
      ],
      "metadata": {
        "id": "X-I1ca4UOWvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_over_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
      ],
      "metadata": {
        "id": "_Pc967NRPCCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "i7-AHM63PFQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_over = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_over), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "ohPOn42sPFbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_over_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pzQlUim2PFkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_over_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall Curve with Class weights\")"
      ],
      "metadata": {
        "id": "Pdo5IjvTP9tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forst Undersampling ##\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "rf_under_pipeline = make_pipeline(RandomUnderSampler(random_state=42),\n",
        "                              RandomForestClassifier(n_estimators=100, random_state=13))\n"
      ],
      "metadata": {
        "id": "KdBCR7MsP9yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the hyperparameter grid for Random Forest\n",
        "params = {\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__random_state': [13]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_under_rf = GridSearchCV(\n",
        "    rf_under_pipeline,  # Your pipeline with Random Forest\n",
        "    param_grid=params,  # The hyperparameter grid\n",
        "    cv=cv,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_under_rf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "7a1sTz2pfhQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_under_rf.best_params_)\n",
        "print('Best score:', grid_under_rf.best_score_)"
      ],
      "metadata": {
        "id": "lOrFkfADfhgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_under_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
      ],
      "metadata": {
        "id": "u2CE6kTIfhpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "2mVzYhbZLxXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "C5f-ZqkwIeOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_under_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IMC6P1gEjPKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_under_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall Curve with Class weights\")"
      ],
      "metadata": {
        "id": "MyWbGAfNjPO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest SMOTE ##\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "rf_smote_pipeline = make_pipeline(SMOTE(random_state=42),\n",
        "                              RandomForestClassifier(n_estimators=100, random_state=13))\n"
      ],
      "metadata": {
        "id": "KDvOjkRzjPUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid for Random Forest\n",
        "params = {\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__random_state': [13]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_smote_rf = GridSearchCV(\n",
        "    rf_smote_pipeline,  # Your pipeline with Random Forest\n",
        "    param_grid=params,  # The hyperparameter grid\n",
        "    cv=cv,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_smote_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-5IuLLEuIeXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_smote_rf.best_params_)\n",
        "print('Best score:', grid_smote_rf.best_score_)"
      ],
      "metadata": {
        "id": "lfw-gS03l5F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_smote_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
      ],
      "metadata": {
        "id": "_xi0djrpmvXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "ZQZEJVgamvXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_smote = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_smote), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "1ZZwJsTIl5On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_smote_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OnT092gZl5VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_smote_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall Curve with Class weights\")"
      ],
      "metadata": {
        "id": "6aG6Xr2mmTpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Optuna - hyperparameter tuning for XGBoost\n",
        "!pip3 install optuna"
      ],
      "metadata": {
        "id": "59F4qQsUoEiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import optuna"
      ],
      "metadata": {
        "id": "kGKb9_WAoLub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## XGBoost - Imbalanced ##\n",
        "classifier = XGBClassifier()"
      ],
      "metadata": {
        "id": "_AVwjChU4WSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Optuna for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Fit the model\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "kL64v_vZyjl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='maximize')"
      ],
      "metadata": {
        "id": "om3_e1Bbnm23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "id": "-8ivgnDcoU08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "_w2ni61IzFFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of finished trials: {}'.format(len(study.trials)))\n",
        "print('Best trial:')\n",
        "\n",
        "print('  Value: {:.2f}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {:.3f}'.format(key, value))"
      ],
      "metadata": {
        "id": "IxYVsZxQ9hxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = trial.params"
      ],
      "metadata": {
        "id": "3IuV1KwvywmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(**params)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "K0LcM8Yuywvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "hev61XRLzPLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "gr_CEBMlzRB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "xgb_matrix = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(xgb_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "ESxiWctv_81J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"XGBoost, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F80PWH-H_81J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    model, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "iP_NCd9UzaHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## XGboost Random Undersampling ##\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Apply Random Undersampling to the training data\n",
        "    sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_train_rus, y_train_rus = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Fit the model on the resampled data\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train_rus, y_train_rus)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "6LTe2oIilZ6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "qQpblYXzEatd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "BkCGMVrzK5fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_rus = trial.params"
      ],
      "metadata": {
        "id": "tuuu4m2EGFi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_under = XGBClassifier(**params_rus)\n",
        "XGB_under.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Liq2YZeeGQGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = XGB_under.predict(X_test)"
      ],
      "metadata": {
        "id": "m52rzGjtGfYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "fX5t_o8-Gfgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_xgb_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_xgb_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "2XtZE_Y1Gp1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = XGB_under.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"XGBoost, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bjv0UcC-HD3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    XGB_under, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "T0N_k_YGHSRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## XGboost Random Oversampling ##\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Apply Random Oversampling to the training data\n",
        "    sampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_train_ros, y_train_ros = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Fit the model on the resampled data\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train_ros, y_train_ros)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "v9W7lRdJHD_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "best_params = study.best_params\n",
        "formatted_params = {key: f\"{value:.3f}\" for key, value in best_params.items()}\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key, value in formatted_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4vFiw1TLPJ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "uXCapNc-PJ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_ros = trial.params"
      ],
      "metadata": {
        "id": "0agVPgJcPJ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_over = XGBClassifier(**params_ros)\n",
        "XGB_over.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "R9l4EhSCPJ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_xgb_over = XGB_over.predict(X_test)\n",
        "print(confusion_matrix(y_test,predict_xgb_over))\n",
        "print(classification_report(y_test,predict_xgb_over))"
      ],
      "metadata": {
        "id": "ayQvtjOcipJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,predict_xgb_over)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "nUd9RjyzjWij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = XGB_over.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7_UZlWpkjWis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    XGB_over, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "Q6ob559ojWis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## XGboost SMOTE ##\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Apply Random Oversampling to the training data\n",
        "    sampler = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "    X_train_smote, y_train_smote = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Fit the model on the resampled data\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "urToH-jxipW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "best_params = study.best_params\n",
        "formatted_params = {key: f\"{value:.3f}\" for key, value in best_params.items()}\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key, value in formatted_params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "id": "VfZ7A2A0ipc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "JP-m5QDeC_DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_smote = trial.params"
      ],
      "metadata": {
        "id": "vhEwgnNjC_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_smote = XGBClassifier(**params_smote)\n",
        "XGB_smote.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "X3JC9XN8C_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = XGB_smote.predict(X_test)"
      ],
      "metadata": {
        "id": "l_VXmL-WC_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "yxLVvfsFC_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_xgb_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_xgb_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "q1pPc51TC_Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = XGB_smote.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"XGBoost, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hf2MLei8C_Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    XGB_smote, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "bRPbH2HwC_Dc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}