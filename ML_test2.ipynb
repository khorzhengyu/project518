{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khorzhengyu/project518/blob/main/ML_test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b9e37ec9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "id": "b9e37ec9"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "23ab8b2b"
      },
      "outputs": [],
      "source": [
        "#modules for data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "#modules for model building\n",
        "#algorithms for sampling\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#modules for machine learning algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import collections\n",
        "from sklearn import datasets, metrics, preprocessing\n",
        "\n",
        "\n",
        "# modules for evaluation\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import Booster\n",
        "from xgboost import DMatrix"
      ],
      "id": "23ab8b2b"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCwyW8kIdln5",
        "outputId": "c68258ed-a33f-4dcb-d1b3-9f668a019602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "SCwyW8kIdln5"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "czArorTRnf3R"
      },
      "outputs": [],
      "source": [
        "# read the csv file\n",
        "df_train= pd.read_csv('gdrive/My Drive/fraudTrain.csv')\n",
        "df_test = pd.read_csv('gdrive/My Drive/fraudTest.csv')"
      ],
      "id": "czArorTRnf3R"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fpg-TzBGmx3J"
      },
      "outputs": [],
      "source": [
        "#concatenating the two datasets together\n",
        "df = pd.concat([df_train,df_test], ignore_index =True)"
      ],
      "id": "Fpg-TzBGmx3J"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q_lCdqDjSjBs"
      },
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.replace('_', '')"
      ],
      "id": "q_lCdqDjSjBs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqlxhnRjm43A"
      },
      "outputs": [],
      "source": [
        "# taking 90000 data for computation\n",
        "df = df.sample(frac=1, random_state=2)\n",
        "df = df.head(n=90000)\n",
        "df.isfraud.value_counts()"
      ],
      "id": "VqlxhnRjm43A"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yk93KcSyCatB"
      },
      "outputs": [],
      "source": [
        "# Adding a date variable in the format: YYYY-MM-DD\n",
        "df['transdatetranstime'] = pd.to_datetime(df['transdatetranstime'])\n",
        "df['transdate'] = df['transdatetranstime'].dt.date\n",
        "df['transdate'] = pd.to_datetime(df['transdate'])"
      ],
      "id": "yk93KcSyCatB"
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wCgx4HQrnJet"
      },
      "id": "wCgx4HQrnJet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W38oolu5AfO2"
      },
      "outputs": [],
      "source": [
        "#sort by transaction date\n",
        "df.sort_values(by = 'transdate', inplace = True)"
      ],
      "id": "W38oolu5AfO2"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9dP7Ks_nGOpU"
      },
      "outputs": [],
      "source": [
        "# Adding 'recnum' as index\n",
        "df['recnum'] = range(1, len(df) + 1)"
      ],
      "id": "9dP7Ks_nGOpU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40459a52"
      },
      "outputs": [],
      "source": [
        "#remove the first unused column\n",
        "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "df.head()"
      ],
      "id": "40459a52"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h49O5qZ2DEF"
      },
      "outputs": [],
      "source": [
        "print('Rows     :', df.shape[0])\n",
        "print('Columns  :', df.shape[1])\n",
        "print('\\nFeatures:\\n', df.columns.tolist())\n",
        "print('\\nMissing values :', df.isnull().sum().values.sum())\n",
        "print('\\nUnique value:\\n', df.nunique())"
      ],
      "id": "7h49O5qZ2DEF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUaUgq2EnAI6"
      },
      "outputs": [],
      "source": [
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# df.loc[df['cc_num'] == 630451534402]"
      ],
      "id": "BUaUgq2EnAI6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXgVnpVBsZ_P"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ],
      "id": "IXgVnpVBsZ_P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wrCYXLJrFsA"
      },
      "outputs": [],
      "source": [
        "# Creating candidates variables ####\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "# Make a copy of DataFrame to avoid modifying the original data\n",
        "df_var = deepcopy(df)\n",
        "\n",
        "# Convert values to string type for selected columns\n",
        "cols_convert = ['ccnum', 'merchlat','merchlong']\n",
        "for item in cols_convert:\n",
        "    df_var[item] = df_var[item].astype(str)\n",
        "\n",
        "# Print data types of the columns in the modified DataFrame\n",
        "print(df_var.dtypes)"
      ],
      "id": "2wrCYXLJrFsA"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6kOFqzByyJ8f"
      },
      "outputs": [],
      "source": [
        "# Make variable combos\n",
        "df_var['card-merchant'] = df_var['ccnum'] + df_var['merchant']\n",
        "#df_var['card-merchant-location'] = df_var['cc_num'] + df_var['merch_lat'] + df_var['merch_long']"
      ],
      "id": "6kOFqzByyJ8f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttRIUatAwN4S"
      },
      "outputs": [],
      "source": [
        "df_var.info()"
      ],
      "id": "ttRIUatAwN4S"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OOaI1mi6X9IS"
      },
      "outputs": [],
      "source": [
        "df_var['ccnum'] = df_var['ccnum'].astype(int)\n",
        "df_var['merchlat'] = df_var['merchlat'].astype(float)\n",
        "df_var['merchlong'] = df_var['merchlong'].astype(float)"
      ],
      "id": "OOaI1mi6X9IS"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sKDpbjzOylqP"
      },
      "outputs": [],
      "source": [
        "## Day-since variable\n",
        "# Creating the function for the variable\n",
        "def ds(dataframe, g1, g2, name):\n",
        "#     'Helps with calculating the day since variables'\n",
        "    day_since = dataframe.groupby(g1)[g1].first()\n",
        "    day_since = day_since.rename_axis(['None' for i in range(len(g1))]).groupby(g2).diff()\n",
        "    day_since.columns = [name]\n",
        "    day_since = day_since.rename_axis(g1)\n",
        "    day_since[name] = day_since[name].dt.days.fillna(0)\n",
        "    day_since = day_since.reset_index()\n",
        "    return day_since"
      ],
      "id": "sKDpbjzOylqP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8EFJMeg3uHn"
      },
      "outputs": [],
      "source": [
        "# Creating Day-Since Variable\n",
        "start_daySince = pd.datetime.now()\n",
        "day_card = ds(df_var, ['ccnum', 'transdate'], 'ccnum', 'card_daysSince')\n",
        "print(\"done!\", pd.datetime.now()-start_daySince)\n",
        "day_card"
      ],
      "id": "y8EFJMeg3uHn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJqGmdVBJnO7"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows',None)\n",
        "day_card.head(10)"
      ],
      "id": "YJqGmdVBJnO7"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D31yzkjfbr_X"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('max_columns')\n",
        "pd.reset_option('max_rows')"
      ],
      "id": "D31yzkjfbr_X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p0XiIbGyUQ3"
      },
      "outputs": [],
      "source": [
        "# day_card"
      ],
      "id": "2p0XiIbGyUQ3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzN8TXeB1mah"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "time_ds_all=datetime.now()\n",
        "# Calculate the Days Since variables for the required columns\n",
        "ds_cols = ['ccnum','merchant','card-merchant']\n",
        "\n",
        "ds_dict={}\n",
        "for col in ds_cols:\n",
        "    curr_time=datetime.now()\n",
        "    curr_name = 'daysSince_' + col\n",
        "\n",
        "    # Calculate the days-since variable (ds) and assign it to a global variable (curr_name)\n",
        "    vars()[curr_name] = ds(df_var, [col, 'transdate'], col, col+'_daysSince')\n",
        "    ds_dict[curr_name] = vars()[curr_name] # Save results to a dictionary\n",
        "\n",
        "    print(\"Done with:\", col, \"; Time:\", datetime.now()-curr_time)\n",
        "\n",
        "print(\"DONE!\", datetime.now()-time_ds_all)"
      ],
      "id": "SzN8TXeB1mah"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJLDiotH3v0p",
        "outputId": "9e4a48bc-3f0c-4e6e-f0b6-e1be05c133dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict_keys'>\n"
          ]
        }
      ],
      "source": [
        "ds_dict.keys()\n",
        "print(type(ds_dict.keys()))"
      ],
      "id": "xJLDiotH3v0p"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tvYMJtqE3w91"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the main DataFrame to avoid modifying the original data\n",
        "df_ds = df_var.copy()\n",
        "\n",
        "# Merge the days-since variables with the main dataset\n",
        "for item in ds_dict.keys():\n",
        "    col_variable = item.split('_')[1]\n",
        "    df_ds = pd.merge(df_ds, vars()[item], how='left', left_on=[col_variable,'transdate'], right_on=[col_variable,'transdate'])"
      ],
      "id": "tvYMJtqE3w91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRg7Ms_L0Md3"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows',None)\n",
        "df_ds.loc[df_ds['merchant'] == 'fraud_Kuvalis Ltd']"
      ],
      "id": "aRg7Ms_L0Md3"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IZno93hub1uv"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('max_columns')\n",
        "pd.reset_option('max_rows')"
      ],
      "id": "IZno93hub1uv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKflIxL-qPMV"
      },
      "outputs": [],
      "source": [
        "df_ds.to_csv('df_daysSince.csv')"
      ],
      "id": "cKflIxL-qPMV"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS7nN49x1ht4",
        "outputId": "d4659b5a-599c-40db-9998-9b1d1f98e323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copy time 0:00:00.028406\n",
            "first loop 0:00:00.010466\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "# Create Columns for the Necessary Time Periods ##\n",
        "#This makes new columns for the various time periods.\n",
        "# Make a list of variable combinations to iterate through and create time-related variables\n",
        "var_combos = ['ccnum','merchant','card-merchant']\n",
        "\n",
        "# Create column names\n",
        "time_list = [0,1,3,7,14,30]\n",
        "time_joined =[]\n",
        "for time in time_list:\n",
        "    time_joined.append('join_ts_'+str(time))\n",
        "\n",
        "start_copy = datetime.now()\n",
        "df_var1 = deepcopy(df_var)\n",
        "df_var2 = deepcopy(df_var)\n",
        "print('copy time', datetime.now()-start_copy)\n",
        "\n",
        "# Creating columns for time\n",
        "start_loop=datetime.now()\n",
        "for time in time_list:\n",
        "    temp_endTime = 'join_ts_' + str(time)\n",
        "    df_var2[temp_endTime] = df_var2['transdate'] + dt.timedelta(days = time)\n",
        "print('first loop', datetime.now()-start_loop)"
      ],
      "id": "lS7nN49x1ht4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-whh_mOOeNSf"
      },
      "outputs": [],
      "source": [
        "# Create the Frequency Candidate Variables ##\n",
        "start_loop2=datetime.now()\n",
        "df_final = deepcopy(df_var.set_index('recnum'))\n",
        "\n",
        "for item in var_combos:\n",
        "    df_var3 = df_var1[['recnum','transdate',item]]\n",
        "    temp_list = time_joined + [item]\n",
        "    df_var4 = df_var2[temp_list + ['recnum']].copy()\n",
        "    df_var4.rename(columns={'recnum':'recnum2'},inplace=True) # this causes a warning to arise\n",
        "\n",
        "    df_temp = pd.merge(df_var3, df_var4, left_on=[item], right_on=[item])\n",
        "\n",
        "    for time in time_list:\n",
        "        temp_endTime = 'join_ts_' + str(time)\n",
        "        df2_temp = df_temp[(df_temp['transdate'] <= df_temp[temp_endTime]) & (df_temp['recnum2'] <= df_temp['recnum'])]\n",
        "        temp_groupby = df2_temp[['recnum','transdate']].groupby('recnum')\n",
        "        temp_name = item + '_' + 'freq' + str(time) + '_'\n",
        "        df_final = pd.merge(df_final, getattr(temp_groupby,'count')().add_prefix(temp_name), left_index=True, right_index=True, how='left')\n",
        "\n",
        "print('second loop', datetime.now()-start_loop2)\n",
        "print(len(df_final.columns))\n",
        "df_final.head()"
      ],
      "id": "-whh_mOOeNSf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkodmrDmX6sJ"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows',None)\n",
        "df_final.loc[df_final['ccnum'] == 6011504998544485]"
      ],
      "id": "bkodmrDmX6sJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcQZ1yGQYo4J"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('max_columns')\n",
        "pd.reset_option('max_rows')"
      ],
      "id": "jcQZ1yGQYo4J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UblaAqrfukxS"
      },
      "outputs": [],
      "source": [
        "# Save the frequency variables\n",
        "df_final.to_csv('df_frequency_vars.csv')"
      ],
      "id": "UblaAqrfukxS"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xEYlFoHva5Q",
        "outputId": "b64d1758-f47e-4975-cf63-f54369f6c176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "third loop 0:00:00.068177\n"
          ]
        }
      ],
      "source": [
        "start_loop3= datetime.now()\n",
        "groupbyvar_denom = deepcopy(var_combos)\n",
        "days_numer = ['0','1']\n",
        "days_denom = ['7','14','30']\n",
        "\n",
        "for b in groupbyvar_denom:\n",
        "    for c in days_numer:\n",
        "        for d in days_denom:\n",
        "            temp = d\n",
        "            df_final[b + '_' + c + '_dayfreq' + '_div_' + d + '_dayfreq' + '_velchange'] = \\\n",
        "                df_final[b + '_freq' + c + '_transdate'] / \\\n",
        "                df_final[b + '_freq' + d + '_transdate'] / float(temp)\n",
        "print('third loop', datetime.now() - start_loop3)"
      ],
      "id": "5xEYlFoHva5Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkaHeOa6leht"
      },
      "outputs": [],
      "source": [
        "# Save the frequency and velocity change variables\n",
        "df_final.to_csv('df_freq_velchange_vars.csv')"
      ],
      "id": "vkaHeOa6leht"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m1eejbXlxU0f"
      },
      "outputs": [],
      "source": [
        "# Reset the index so that the Recnum returns to being a normal column\n",
        "df_final.reset_index(inplace=True)"
      ],
      "id": "m1eejbXlxU0f"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mfBnNfXAxVMo"
      },
      "outputs": [],
      "source": [
        "# Merge df_final (frequency and velocity change) and df_ds (days-since)\n",
        "df_all_vars = pd.merge(df_final, df_ds, on=list(df_var.columns))"
      ],
      "id": "mfBnNfXAxVMo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5BBHw0lPBxm"
      },
      "outputs": [],
      "source": [
        "# pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.max_rows',None)\n",
        "\n",
        "# df_all_vars.loc[df_all_vars['cc_num'] == 4841313716651064]"
      ],
      "id": "D5BBHw0lPBxm"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7TILgt2sxjyO"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "# Optimize Function\n",
        "def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
        "    df[floats] = df[floats].apply(pd.to_numeric, downcast='float')\n",
        "    return df\n",
        "\n",
        "\n",
        "def optimize_ints(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
        "    df[ints] = df[ints].apply(pd.to_numeric, downcast='integer')\n",
        "    return df\n",
        "\n",
        "\n",
        "def optimize_objects(df: pd.DataFrame, datetime_features: List[str]) -> pd.DataFrame:\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        if col not in datetime_features:\n",
        "            num_unique_values = len(df[col].unique())\n",
        "            num_total_values = len(df[col])\n",
        "            if float(num_unique_values) / num_total_values < 0.5:\n",
        "                df[col] = df[col].astype('category')\n",
        "        else:\n",
        "            df[col] = pd.to_datetime(df[col])\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def optimize(df: pd.DataFrame, datetime_features: List[str] = []):\n",
        "    return optimize_floats(optimize_ints(optimize_objects(df, datetime_features)))"
      ],
      "id": "7TILgt2sxjyO"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5S3zNehtxj24"
      },
      "outputs": [],
      "source": [
        "# Make variable combos\n",
        "main_dataset_filled = deepcopy(df)\n",
        "main_dataset_filled['ccnum_|_merchant'] = main_dataset_filled['ccnum'].astype('str') + main_dataset_filled['merchant'].astype('str')\n",
        " # main_dataset_filled['ccnum_|_merchantlocation'] = main_dataset_filled['cc_num'].astype('str') + main_dataset_filled['merch_lat'].astype('str') + main_dataset_filled['merch_long'].astype('str')"
      ],
      "id": "5S3zNehtxj24"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hPJJxxulxj9S"
      },
      "outputs": [],
      "source": [
        "var_combos = ['ccnum','merchant','ccnum_|_merchant']"
      ],
      "id": "hPJJxxulxj9S"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UEwERsv1gKu",
        "outputId": "4fa3f95a-0afc-4cdb-ffce-4b24c4b69113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory percentage used 23.8\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "print('Memory percentage used',psutil.virtual_memory().percent)"
      ],
      "id": "2UEwERsv1gKu"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wYEb_yNF1mnY"
      },
      "outputs": [],
      "source": [
        "time_joined =['join_ts1']\n",
        "for num in time_list:\n",
        "    time_joined.append('join_ts2_'+str(num))"
      ],
      "id": "wYEb_yNF1mnY"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmkwyY4-1s6C",
        "outputId": "13df5d71-6e7b-4492-cb68-35872abee1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed  0:00:00.014351\n"
          ]
        }
      ],
      "source": [
        "# Making time variables\n",
        "start=datetime.now()\n",
        "\n",
        "time_list = [0,1,3,7,14,30]\n",
        "main_dataset_filled['join_ts1']=main_dataset_filled['transdate']\n",
        "# dt_i\n",
        "for dt_i in time_list:\n",
        "    time = 'join_ts2_'+str(dt_i)\n",
        "    main_dataset_filled[time]=main_dataset_filled['transdate'] + dt.timedelta(dt_i)\n",
        "print('Completed ',datetime.now()-start)"
      ],
      "id": "CmkwyY4-1s6C"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "W5uezL932E2B"
      },
      "outputs": [],
      "source": [
        "df_var1 = main_dataset_filled.copy()\n",
        "df_var2 = main_dataset_filled.copy()"
      ],
      "id": "W5uezL932E2B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKHVhUFK2IuS"
      },
      "outputs": [],
      "source": [
        "# 'Cardnum_|_zip_totalamount14_Amount2' = total amount for the past 14 days\n",
        "# for the same cardnumber and zip"
      ],
      "id": "dKHVhUFK2IuS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VceGPTLV2PPW"
      },
      "outputs": [],
      "source": [
        "# Total Amount Variables\n",
        "start_loop2= datetime.now()\n",
        "df_final = deepcopy(main_dataset_filled.set_index('recnum'))\n",
        "\n",
        "for item in var_combos:\n",
        "    df_var3 = df_var1[['recnum','transdate','amt',item]]\n",
        "    temp_list = time_joined + [item]\n",
        "    df_var4 = df_var2[temp_list + ['recnum','amt']].copy()\n",
        "    df_var4.rename(columns={'recnum':'recnum2','amt':'amt2'},inplace=True) # this causes a warning to arise\n",
        "#     df_var4['record2'] = df_var2['record'] # this causes a warning to arise\n",
        "\n",
        "    df_temp = pd.merge(df_var3, df_var4, left_on=[item], right_on=[item])\n",
        "\n",
        "    for time in time_list:\n",
        "        temp_endTime = 'join_ts2_' + str(time)\n",
        "#         df2_temp = df_temp[(df_temp['date'] >= df_temp['join_ts1']) & (df_temp['date'] <= df_temp[temp_endTime])] # Original from TA\n",
        "        df2_temp = df_temp[(df_temp['transdate'] <= df_temp[temp_endTime]) & (df_temp['recnum2'] <= df_temp['recnum'])]\n",
        "\n",
        "        temp_groupby = df2_temp[['recnum','amt2']].groupby('recnum')\n",
        "\n",
        "        temp_name = item + '_' + 'totalamount' + str(time) + '_'\n",
        "        df_final = pd.merge(df_final, getattr(temp_groupby,'sum')().add_prefix(temp_name), left_index=True, right_index=True, how='left')\n",
        "#         break\n",
        "#     break\n",
        "print('second loop', datetime.now()-start_loop2)\n",
        "print(len(df_final.columns))\n",
        "df_final.head()"
      ],
      "id": "VceGPTLV2PPW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tV3xNyV3pk8"
      },
      "outputs": [],
      "source": [
        "# Median amount\n",
        "start_loop2= datetime.now()\n",
        "# df_final = deepcopy(main_dataset_filled.set_index('Recnum'))\n",
        "\n",
        "for item in var_combos:\n",
        "    df_var3 = df_var1[['recnum','transdate','amt',item]]\n",
        "    temp_list = time_joined + [item]\n",
        "    df_var4 = df_var2[temp_list + ['recnum','amt']].copy()\n",
        "    df_var4.rename(columns={'recnum':'recnum2','amt':'amt2'},inplace=True)\n",
        "#     df_var4['record2'] = df_var2['record'] # this causes a warning to arise\n",
        "\n",
        "    df_temp = pd.merge(df_var3, df_var4, left_on=[item], right_on=[item])\n",
        "\n",
        "    for time in time_list:\n",
        "        temp_endTime = 'join_ts2_' + str(time)\n",
        "#         df2_temp = df_temp[(df_temp['date'] >= df_temp['join_ts1']) & (df_temp['date'] <= df_temp[temp_endTime])] # Original from TA\n",
        "        df2_temp = df_temp[(df_temp['transdate'] <= df_temp[temp_endTime]) & (df_temp['recnum2'] <= df_temp['recnum'])]\n",
        "\n",
        "        temp_groupby = df2_temp[['recnum','amt2']].groupby('recnum')\n",
        "\n",
        "        temp_name = item + '_' + 'median' + str(time) + '_'\n",
        "        df_final = pd.merge(df_final, getattr(temp_groupby,'median')().add_prefix(temp_name), left_index=True, right_index=True, how='left')\n",
        "#         break\n",
        "#     break\n",
        "print('second loop', datetime.now()-start_loop2)\n",
        "print(len(df_final.columns))\n",
        "df_final.head()"
      ],
      "id": "-tV3xNyV3pk8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR1BJtyz6J68"
      },
      "outputs": [],
      "source": [
        "# Mean Variables\n",
        "\n",
        "start_loop2= datetime.now()\n",
        "# df_final = deepcopy(main_dataset_filled.set_index('Recnum'))\n",
        "\n",
        "for item in var_combos:\n",
        "    df_var3 = df_var1[['recnum','transdate','amt',item]]\n",
        "    temp_list = time_joined + [item]\n",
        "    df_var4 = df_var2[temp_list + ['recnum','amt']].copy()\n",
        "    df_var4.rename(columns={'recnum':'recnum2','amt':'amt2'},inplace=True)\n",
        "#     df_var4['record2'] = df_var2['record'] # this causes a warning to arise\n",
        "\n",
        "    df_temp = pd.merge(df_var3, df_var4, left_on=[item], right_on=[item])\n",
        "\n",
        "    for time in time_list:\n",
        "        temp_endTime = 'join_ts2_' + str(time)\n",
        "#         df2_temp = df_temp[(df_temp['date'] >= df_temp['join_ts1']) & (df_temp['date'] <= df_temp[temp_endTime])] # Original from TA\n",
        "        df2_temp = df_temp[(df_temp['transdate'] <= df_temp[temp_endTime]) & (df_temp['recnum2'] <= df_temp['recnum'])]\n",
        "\n",
        "        temp_groupby = df2_temp[['recnum','amt2']].groupby('recnum')\n",
        "\n",
        "        temp_name = item + '_' + 'mean' + str(time) + '_'\n",
        "        df_final = pd.merge(df_final, getattr(temp_groupby,'mean')().add_prefix(temp_name), left_index=True, right_index=True, how='left')\n",
        "#         break\n",
        "#     break\n",
        "print('second loop', datetime.now()-start_loop2)\n",
        "print(len(df_final.columns))\n",
        "df_final.head()"
      ],
      "id": "VR1BJtyz6J68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtUl38Lh7D-w"
      },
      "outputs": [],
      "source": [
        " # Max Variables\n",
        "start_loop2= datetime.now()\n",
        "# df_final = deepcopy(main_dataset_filled.set_index('Recnum'))\n",
        "\n",
        "for item in var_combos:\n",
        "    df_var3 = df_var1[['recnum','transdate','amt',item]]\n",
        "    temp_list = time_joined + [item]\n",
        "    df_var4 = df_var2[temp_list + ['recnum','amt']].copy()\n",
        "    df_var4.rename(columns={'recnum':'recnum2','amt':'amt2'},inplace=True)\n",
        "#     df_var4['record2'] = df_var2['record'] # this causes a warning to arise\n",
        "\n",
        "    df_temp = pd.merge(df_var3, df_var4, left_on=[item], right_on=[item])\n",
        "\n",
        "    for time in time_list:\n",
        "        temp_endTime = 'join_ts2_' + str(time)\n",
        "#         df2_temp = df_temp[(df_temp['date'] >= df_temp['join_ts1']) & (df_temp['date'] <= df_temp[temp_endTime])] # Original from TA\n",
        "        df2_temp = df_temp[(df_temp['transdate'] <= df_temp[temp_endTime]) & (df_temp['recnum2'] <= df_temp['recnum'])]\n",
        "\n",
        "        temp_groupby = df2_temp[['recnum','amt2']].groupby('recnum')\n",
        "\n",
        "        temp_name = item + '_' + 'max' + str(time) + '_'\n",
        "        df_final = pd.merge(df_final, getattr(temp_groupby,'max')().add_prefix(temp_name), left_index=True, right_index=True, how='left')\n",
        "#         break\n",
        "#     break\n",
        "print('second loop', datetime.now()-start_loop2)\n",
        "print(len(df_final.columns))\n",
        "df_final.head()"
      ],
      "id": "vtUl38Lh7D-w"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_dg6i0cX7EEG"
      },
      "outputs": [],
      "source": [
        "df_final = optimize(df_final,['transdate'])"
      ],
      "id": "_dg6i0cX7EEG"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNXit5KpdREV",
        "outputId": "5073bae5-ddea-4c85-a1d4-72cb569f9a16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['transdatetranstime',\n",
              " 'ccnum',\n",
              " 'merchant',\n",
              " 'category',\n",
              " 'amt',\n",
              " 'first',\n",
              " 'last',\n",
              " 'gender',\n",
              " 'street',\n",
              " 'city',\n",
              " 'state',\n",
              " 'zip',\n",
              " 'lat',\n",
              " 'long',\n",
              " 'citypop',\n",
              " 'job',\n",
              " 'dob',\n",
              " 'transnum',\n",
              " 'unixtime',\n",
              " 'merchlat',\n",
              " 'merchlong',\n",
              " 'isfraud',\n",
              " 'transdate',\n",
              " 'ccnum_|_merchant',\n",
              " 'join_ts1',\n",
              " 'join_ts2_0',\n",
              " 'join_ts2_1',\n",
              " 'join_ts2_3',\n",
              " 'join_ts2_7',\n",
              " 'join_ts2_14',\n",
              " 'join_ts2_30',\n",
              " 'ccnum_totalamount0_amt2',\n",
              " 'ccnum_totalamount1_amt2',\n",
              " 'ccnum_totalamount3_amt2',\n",
              " 'ccnum_totalamount7_amt2',\n",
              " 'ccnum_totalamount14_amt2',\n",
              " 'ccnum_totalamount30_amt2',\n",
              " 'merchant_totalamount0_amt2',\n",
              " 'merchant_totalamount1_amt2',\n",
              " 'merchant_totalamount3_amt2',\n",
              " 'merchant_totalamount7_amt2',\n",
              " 'merchant_totalamount14_amt2',\n",
              " 'merchant_totalamount30_amt2',\n",
              " 'ccnum_|_merchant_totalamount0_amt2',\n",
              " 'ccnum_|_merchant_totalamount1_amt2',\n",
              " 'ccnum_|_merchant_totalamount3_amt2',\n",
              " 'ccnum_|_merchant_totalamount7_amt2',\n",
              " 'ccnum_|_merchant_totalamount14_amt2',\n",
              " 'ccnum_|_merchant_totalamount30_amt2',\n",
              " 'ccnum_median0_amt2',\n",
              " 'ccnum_median1_amt2',\n",
              " 'ccnum_median3_amt2',\n",
              " 'ccnum_median7_amt2',\n",
              " 'ccnum_median14_amt2',\n",
              " 'ccnum_median30_amt2',\n",
              " 'merchant_median0_amt2',\n",
              " 'merchant_median1_amt2',\n",
              " 'merchant_median3_amt2',\n",
              " 'merchant_median7_amt2',\n",
              " 'merchant_median14_amt2',\n",
              " 'merchant_median30_amt2',\n",
              " 'ccnum_|_merchant_median0_amt2',\n",
              " 'ccnum_|_merchant_median1_amt2',\n",
              " 'ccnum_|_merchant_median3_amt2',\n",
              " 'ccnum_|_merchant_median7_amt2',\n",
              " 'ccnum_|_merchant_median14_amt2',\n",
              " 'ccnum_|_merchant_median30_amt2',\n",
              " 'ccnum_mean0_amt2',\n",
              " 'ccnum_mean1_amt2',\n",
              " 'ccnum_mean3_amt2',\n",
              " 'ccnum_mean7_amt2',\n",
              " 'ccnum_mean14_amt2',\n",
              " 'ccnum_mean30_amt2',\n",
              " 'merchant_mean0_amt2',\n",
              " 'merchant_mean1_amt2',\n",
              " 'merchant_mean3_amt2',\n",
              " 'merchant_mean7_amt2',\n",
              " 'merchant_mean14_amt2',\n",
              " 'merchant_mean30_amt2',\n",
              " 'ccnum_|_merchant_mean0_amt2',\n",
              " 'ccnum_|_merchant_mean1_amt2',\n",
              " 'ccnum_|_merchant_mean3_amt2',\n",
              " 'ccnum_|_merchant_mean7_amt2',\n",
              " 'ccnum_|_merchant_mean14_amt2',\n",
              " 'ccnum_|_merchant_mean30_amt2',\n",
              " 'ccnum_max0_amt2',\n",
              " 'ccnum_max1_amt2',\n",
              " 'ccnum_max3_amt2',\n",
              " 'ccnum_max7_amt2',\n",
              " 'ccnum_max14_amt2',\n",
              " 'ccnum_max30_amt2',\n",
              " 'merchant_max0_amt2',\n",
              " 'merchant_max1_amt2',\n",
              " 'merchant_max3_amt2',\n",
              " 'merchant_max7_amt2',\n",
              " 'merchant_max14_amt2',\n",
              " 'merchant_max30_amt2',\n",
              " 'ccnum_|_merchant_max0_amt2',\n",
              " 'ccnum_|_merchant_max1_amt2',\n",
              " 'ccnum_|_merchant_max3_amt2',\n",
              " 'ccnum_|_merchant_max7_amt2',\n",
              " 'ccnum_|_merchant_max14_amt2',\n",
              " 'ccnum_|_merchant_max30_amt2']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "all_columns = df_final.columns.tolist()\n",
        "all_columns"
      ],
      "id": "KNXit5KpdREV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEVzB7Ww7oXo"
      },
      "outputs": [],
      "source": [
        "start_column = df_final.columns.get_loc('ccnum_totalamount0_amt2')\n",
        "for col_names in df_final.columns[start_column:]:\n",
        "#  the number of days in each column is the column index mod length of time array\n",
        "    number_index = time_list[(df_final.columns.get_loc(col_names)%len(time_list))-1]\n",
        "#     we split the name of the array to keep the first part and use it to create the new column names\n",
        "    new_col_name = col_names.split(str(number_index))[0]+'_'+str(number_index)+'_actual'\n",
        "#     print(new_col_name)\n",
        "    df_final[new_col_name] = df_final[\"amt\"]/df_final[col_names]"
      ],
      "id": "iEVzB7Ww7oXo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu05XC8fHmj8"
      },
      "outputs": [],
      "source": [
        "# # df_final['Merchnum'][8]\n",
        "# pd.set_option('display.max_rows', None)\n",
        "# df_final[['ccnum']].loc[df_final['ccnum_|_merchant_totalamount_7_actual'] != 1]\n",
        "\n",
        "# # np.array([df_final.columns == 'Merchnum_max_7_actual']).sum()\n",
        "# # df_final['Merch']"
      ],
      "id": "Gu05XC8fHmj8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPumQ4H_71v0"
      },
      "outputs": [],
      "source": [
        "# Amount Velocity Change Variables\n",
        "# iterates through  the variables created with the var_combos columns and finds the relative\n",
        "# velocity variables. Divides the number of days of days_numer array with the days_denom one\n",
        "start_loop3= datetime.now()\n",
        "groupbyvar_denom = deepcopy(var_combos)\n",
        "days_numer = ['0','1']\n",
        "days_denom = ['7','14','30']\n",
        "# ccnum_totalamount0_amt2_/_ccnum_totalamount3_amt2\n",
        "for b in groupbyvar_denom:\n",
        "    for c in days_numer:\n",
        "        for d in days_denom:\n",
        "            temp = d\n",
        "            df_final[b + '_' + c + '_dayamount' + '_div_' + d + '_dayamount' + '_velchange'] = \\\n",
        "                df_final[b+'_totalamount'+c+'_amt2'] / \\\n",
        "                df_final[b+'_totalamount'+d+'_amt2'] / float(temp)\n",
        "print('third loop', datetime.now() - start_loop3)"
      ],
      "id": "gPumQ4H_71v0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny-j863W1LsO"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('max_columns')\n",
        "pd.reset_option('max_rows')"
      ],
      "id": "ny-j863W1LsO"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "7WVQHre2wci0"
      },
      "outputs": [],
      "source": [
        "df_all_vars = optimize(df_all_vars)"
      ],
      "id": "7WVQHre2wci0"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uGdHiMYA3Y9R"
      },
      "outputs": [],
      "source": [
        "# df_all_vars contains frequency variables, velocity change variables, and days since variables\n",
        "# merge1_df contains amount variables, velocity change amount variables, and benford's law variables\n",
        "df_all_vars_final = pd.merge(df_all_vars, df_final, how='left',left_on=df.columns.to_list(),right_on=df.columns.to_list())"
      ],
      "id": "uGdHiMYA3Y9R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_QMUp5jqZdg"
      },
      "outputs": [],
      "source": [
        "df_all_vars_final = df_all_vars_final.drop(columns=['card-merchant', 'join_ts1', 'join_ts2_0',\n",
        "       'join_ts2_1', 'join_ts2_3', 'join_ts2_7', 'join_ts2_14',\n",
        "       'join_ts2_30'])\n",
        "df_all_vars_final.head()"
      ],
      "id": "9_QMUp5jqZdg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3q2EfAgKttJ"
      },
      "outputs": [],
      "source": [
        "#### Examples ########\n",
        "#\n",
        "#\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Selecting specific columns\n",
        "selected_columns = ['ccnum', 'transdate', 'amt', 'ccnum_freq0_transdate', 'ccnum_freq1_transdate' ,'ccnum_freq3_transdate', 'ccnum_freq7_transdate']\n",
        "filtered_df = df_all_vars_final[selected_columns]\n",
        "\n",
        "# Filtering rows based on ccnum\n",
        "ccnum_filter = 6011504998544485\n",
        "filtered_df = filtered_df[filtered_df['ccnum'] == ccnum_filter]\n",
        "\n",
        "# Define the range of indices you want to show\n",
        "start_index = 5\n",
        "end_index = 200\n",
        "\n",
        "# Filter rows within the specified index range\n",
        "filtered_df = filtered_df.iloc[start_index:end_index+1]\n",
        "\n",
        "filtered_df\n"
      ],
      "id": "J3q2EfAgKttJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLztoDMpGxFf"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Selecting specific columns\n",
        "selected_columns = ['recnum', 'ccnum', 'merchant', 'transdate', 'amt', 'ccnum_totalamount0_amt2', 'ccnum_|_merchant_totalamount7_amt2' ,'ccnum_|_merchant_totalamount_7_actual']\n",
        "filtered_df = df_all_vars_final[selected_columns]\n",
        "\n",
        "# Filtering rows based on ccnum\n",
        "ccnum_filter = 370877495212014\n",
        "filtered_df = filtered_df[filtered_df['ccnum'] == ccnum_filter]\n",
        "\n",
        "filtered_df\n"
      ],
      "id": "HLztoDMpGxFf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq_GzZA3ioqs"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Selecting specific columns\n",
        "selected_columns = ['recnum', 'ccnum', 'transdate', 'amt', 'ccnum_totalamount0_amt2', 'ccnum_totalamount7_amt2' ,'ccnum_0_dayamount_div_7_dayamount_velchange']\n",
        "filtered_df = df_all_vars_final[selected_columns]\n",
        "\n",
        "# Filtering rows based on ccnum\n",
        "ccnum_filter = 370877495212014\n",
        "filtered_df = filtered_df[filtered_df['ccnum'] == ccnum_filter]\n",
        "\n",
        "filtered_df\n"
      ],
      "id": "uq_GzZA3ioqs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HInkhQHP-EMn"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Selecting specific columns\n",
        "selected_columns = ['recnum', 'ccnum', 'transdate', 'amt', 'ccnum_freq0_transdate', 'ccnum_freq7_transdate' ,'ccnum_0_dayfreq_div_7_dayfreq_velchange']\n",
        "filtered_df = df_all_vars_final[selected_columns]\n",
        "\n",
        "# Filtering rows based on ccnum\n",
        "ccnum_filter = 370877495212014\n",
        "filtered_df = filtered_df[filtered_df['ccnum'] == ccnum_filter]\n",
        "\n",
        "filtered_df\n"
      ],
      "id": "HInkhQHP-EMn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m5EbWRquuKY"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('max_columns')\n",
        "pd.reset_option('max_rows')"
      ],
      "id": "-m5EbWRquuKY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8hAVrrP7TEM"
      },
      "outputs": [],
      "source": [
        "# print(\"Columns in df_final:\", df_final.columns)\n",
        "# print(\"Columns in df_all_var:\", df_all_vars.columns)"
      ],
      "id": "C8hAVrrP7TEM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5JBJiuZ70lw"
      },
      "outputs": [],
      "source": [
        "df = df_all_vars_final.copy()\n",
        "df.head()"
      ],
      "id": "D5JBJiuZ70lw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "177becb6"
      },
      "outputs": [],
      "source": [
        "results = pd.concat(\n",
        "    [\n",
        "        df['amt'].describe(percentiles=[0.5, 0.95, 0.999]).reset_index().rename(columns={'index': 'Row Type', 'amt': 'Overall Amt Distribution'}).round(3),\n",
        "        df.loc[df['isfraud'] == 0, ['amt']].describe(percentiles=[0.5, 0.95, 0.999]).reset_index(drop=1).rename(columns={'amt': 'Non-Fraud Amt Distribution'}).round(3),\n",
        "        df.loc[df['isfraud'] == 1, ['amt']].describe(percentiles=[0.5, 0.95, 0.999]).reset_index(drop=1).rename(columns={'amt': 'Fraud Amt Distribution'}).round(3)\n",
        "    ], axis=1)\n",
        "results"
      ],
      "id": "177becb6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_h_BUNzas6r"
      },
      "outputs": [],
      "source": [
        "# all_trans = df.copy()\n",
        "# all_trans['class'] = all_trans['is_fraud'].map({1:'Fraud',0:'Non-Fraud'})\n",
        "# normal = all_trans[df['is_fraud']==0]\n",
        "# fraud = all_trans[df['is_fraud'] == 1]\n",
        "\n",
        "# def stats_by_class(variable):\n",
        "#   stat_grid = all_trans.groupby('class')[variable].agg([np.min,np.max,np.mean,np.median])\n",
        "#   stat_grid = stat_grid.transpose().round(2)\n",
        "#   return stat_grid\n",
        "\n",
        "# def plot_box(data, x,y,title,width = 10, height = 7):\n",
        "#   plt.figure(figsize = [width,height])\n",
        "#   sns.boxplot(data = data, x=x,y=y)\n",
        "#   plt.title(title);\n",
        "\n",
        "# plot_box(all_trans,'class','amt','Distribution of Amount vs Class'); # calling the boxplot function\n",
        "\n",
        "# # stats_by_class('amt') # calling the stats_by class function"
      ],
      "id": "Y_h_BUNzas6r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c612b1f"
      },
      "outputs": [],
      "source": [
        "all_trans = df.copy()\n",
        "all_trans['class'] = all_trans['isfraud'].map({1:'Fraud',0:'Non-Fraud'})\n",
        "normal = all_trans[df['isfraud']==0]\n",
        "fraud = all_trans[df['isfraud'] == 1]\n",
        "\n",
        "def stats_by_class(variable):\n",
        "  stat_grid = all_trans.groupby('class')[variable].agg([np.min,np.max,np.mean,np.median])\n",
        "  stat_grid = stat_grid.transpose().round(2)\n",
        "  return stat_grid\n",
        "\n",
        "def plot_box(data, x,y,title,width = 10, height = 7):\n",
        "  plt.figure(figsize = [width,height])\n",
        "  sns.boxplot(data = data, x=x,y=y)\n",
        "  plt.title(title)\n",
        "\n",
        "#stats_by_class('amt') # calling the stats_by class function\n",
        "\n",
        "plot_box(all_trans,'class','amt','Distribution of Amount vs Class'); # calling the boxplot function\n",
        "\n",
        "fig = plt.subplots(figsize= (15,10))\n",
        "plots = []\n",
        "plots.append(sns.histplot(df[(df.isfraud == 0) & (df.amt <= 2000)].amt, bins=50, ax=plt.subplot(235)))\n",
        "plots.append(sns.histplot(df[(df.isfraud ==1) & (df.amt <= 1500)].amt, bins =50, ax=plt.subplot(236)))\n",
        "\n",
        "plots[0].set_title('Non-Fraud Amt Dist')\n",
        "plots[1].set_title('Fraud Amt Dist')\n",
        "plots[0].set_xlabel('Transaction Amount')\n",
        "plots[1].set_xlabel('Transaction Amount')\n",
        "plots[0].set_ylabel('Number of Transactions')\n",
        "plots[1].set_ylabel('Number of Transactions')\n",
        "plt.show()\n",
        "\n",
        "stats_by_class('amt') # calling the stats_by class function"
      ],
      "id": "2c612b1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53767db0"
      },
      "outputs": [],
      "source": [
        "ax = sns.histplot(x='amt', data=df[df.amt <= 2000], hue='isfraud',stat = 'percent',multiple= 'dodge',common_norm= False, bins = 25)\n",
        "ax.set_ylabel('Percentage in Each Type')\n",
        "ax.set_xlabel('Transaction amount in USD')\n",
        "plt.legend(title='Type', labels = ['Fraud','Non-Fraud'])"
      ],
      "id": "53767db0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiIJWY1Mvc-W"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def calculate_ratio(df, bin_edges):\n",
        "#     # Create a new column 'amount_bin' based on transaction amount bins\n",
        "#     df['amount_bin'] = pd.cut(df['amt'], bins=bin_edges)\n",
        "\n",
        "#     # Group the DataFrame by 'amount_bin' and calculate counts of fraud and non-fraud transactions\n",
        "#     grouped = df.groupby('amount_bin')['is_fraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "#     grouped[0] /= grouped[0].sum()\n",
        "#     grouped[1] /= grouped[1].sum()\n",
        "\n",
        "#     # Calculate the fraud-to-non-fraud ratio for each amount bin\n",
        "#     grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "\n",
        "#     return grouped"
      ],
      "id": "WiIJWY1Mvc-W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr3MNjd-zCxg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_ratio(df, bin_edges):\n",
        "    # Create a new column 'amount_bin' based on transaction amount bins\n",
        "    df['amount_bin'] = pd.cut(df['amt'], bins=bin_edges)\n",
        "\n",
        "    # Group the DataFrame by 'amount_bin' and calculate counts of fraud and non-fraud transactions\n",
        "    grouped = df.groupby('amount_bin')['isfraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "    grouped[0] /= grouped[0].sum()\n",
        "    grouped[1] /= grouped[1].sum()\n",
        "\n",
        "    # Calculate the fraud-to-non-fraud ratio for each amount bin\n",
        "    grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "\n",
        "    return grouped\n",
        "\n",
        "\n",
        "# Define the bin edges for transaction amounts\n",
        "bin_edges = [0, 500, 1000, 2000]\n",
        "bin_labels = ['0-499','500-999','1000-1999']\n",
        "# Calculate the fraud-to-non-fraud ratio for transaction amounts\n",
        "ratio_df = calculate_ratio(df, bin_edges)\n",
        "\n",
        "# Convert the amount_bin intervals to strings\n",
        "ratio_df['amount_bin'] = ratio_df['amount_bin'].astype(str)\n",
        "\n",
        "# Sort the ratio_df DataFrame by fraud_ratio in ascending order\n",
        "ratio_df_sorted = ratio_df.sort_values('fraud_ratio')\n",
        "\n",
        "print (ratio_df_sorted)\n",
        "# Plot all the groups on the bar chart in ascending order\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(ratio_df_sorted['amount_bin'], ratio_df_sorted['fraud_ratio'])\n",
        "plt.xlabel('Transaction Amount ($)')\n",
        "plt.ylabel('Fraud Ratio')\n",
        "plt.title('Fraud Ratio by Transaction Amount')\n",
        "plt.xticks(ratio_df_sorted['amount_bin'],bin_labels, rotation=45)\n",
        "plt.show()\n"
      ],
      "id": "sr3MNjd-zCxg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLj9cXiUcJUw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_ratio(df, bin_edges):\n",
        "    # Create a new column 'amount_bin' based on transaction amount bins\n",
        "    df['amount_bin'] = pd.cut(df['amt'], bins=bin_edges)\n",
        "\n",
        "    # Group the DataFrame by 'amount_bin' and calculate normalized counts of fraud and non-fraud transactions\n",
        "    grouped = df.groupby('amount_bin')['isfraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "    # Normalize both fraud and non-fraud counts\n",
        "    grouped[0] /= grouped[0].sum()\n",
        "    grouped[1] /= grouped[1].sum()\n",
        "\n",
        "    # Calculate the normalized fraud-to-non-fraud ratio for each amount bin\n",
        "    grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "\n",
        "    # Calculate the normalized non-fraud-to-fraud ratio for each amount bin\n",
        "    grouped['non_fraud_ratio'] = grouped[0] / grouped[1]\n",
        "\n",
        "    return grouped\n",
        "\n",
        "@ticker.FuncFormatter\n",
        "def major_formatter(x, pos):\n",
        "    label = str(-x) if x < 0 else str(x)\n",
        "    return label\n",
        "\n",
        "# Define the bin edges for transaction amounts\n",
        "bin_edges = [0, 500, 1000, 2000]\n",
        "bin_labels = ['0-499', '500-999', '1000-1999']\n",
        "# Calculate the normalized fraud-to-non-fraud and non-fraud-to-fraud ratios for transaction amounts\n",
        "ratio_df = calculate_ratio(df, bin_edges)\n",
        "\n",
        "# Convert the amount_bin intervals to strings\n",
        "ratio_df['amount_bin'] = ratio_df['amount_bin'].astype(str)\n",
        "\n",
        "# Sort the ratio_df DataFrame by fraud_ratio in ascending order\n",
        "ratio_df_sorted = ratio_df.sort_values('fraud_ratio')\n",
        "\n",
        "# Normalize the non-fraud ratio in the same way as the fraud ratio\n",
        "max_non_fraud_ratio = ratio_df_sorted['fraud_ratio'].max()\n",
        "ratio_df_sorted['non_fraud_ratio'] /= max_non_fraud_ratio\n",
        "\n",
        "# Plot both normalized fraud and non-fraud ratios on the same bar chart\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Primary y-axis for Fraud Ratio\n",
        "fraud_bars = ax1.barh(ratio_df_sorted['amount_bin'], ratio_df_sorted['fraud_ratio'], label='Fraud', color='blue')\n",
        "ax1.set_xlabel('Fraud Ratio')\n",
        "ax1.xaxis.set_major_formatter(major_formatter)\n",
        "ax1.set_ylabel('Transaction Amount ($)')\n",
        "ax1.set_yticks(ratio_df_sorted['amount_bin'])\n",
        "ax1.set_yticklabels(bin_labels)\n",
        "\n",
        "\n",
        "# Secondary y-axis for Non-Fraud Ratio\n",
        "ax2 = ax1.twiny()\n",
        "non_fraud_bars = ax2.barh(ratio_df_sorted['amount_bin'], -ratio_df_sorted['non_fraud_ratio'], label='Non-Fraud Ratio', alpha=0.7, color='orange')  # Use negative values for non-fraud ratio\n",
        "ax2.set_xlabel('Non-Fraud Ratio')\n",
        "ax2.xaxis.tick_top()\n",
        "ax2.xaxis.set_label_position('top')\n",
        "ax2.xaxis.set_major_formatter(major_formatter)\n",
        "\n",
        "# Display the ratios on the bars with adjusted positions and rotated text\n",
        "for bar, fraud_ratio, non_fraud_ratio in zip(fraud_bars, ratio_df_sorted['fraud_ratio'], ratio_df_sorted['non_fraud_ratio']):\n",
        "    ax1.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f'{fraud_ratio:.2f}', va='center', color='blue', fontsize=10, rotation=90)\n",
        "\n",
        "for bar, non_fraud_ratio in zip(non_fraud_bars, ratio_df_sorted['non_fraud_ratio']):\n",
        "    ax2.text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()/2, f'{non_fraud_ratio:.2f}', va='center', ha='right', color='black', fontsize=10, rotation=90)\n",
        "\n",
        "# Add legend\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
        "\n",
        "plt.title('Fraud and Non-Fraud Ratios by Transaction Amount')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fLj9cXiUcJUw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee0e81c6"
      },
      "outputs": [],
      "source": [
        "# transforming to datetime format\n",
        "df['trans_hour']= df['transdatetranstime'].dt.hour\n",
        "df['trans_day_of_week'] = df['transdatetranstime'].dt.day_name()\n",
        "#df['trans_year_month'] = df['transdatetranstime'].dt.to_period('M')\n",
        "df.head()\n",
        "df.dtypes"
      ],
      "id": "ee0e81c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdbzkXNkvkm8"
      },
      "outputs": [],
      "source": [
        "# def sum_30_day(unixtime, cc_num):\n",
        "#     unixstamp = unixtime\n",
        "#     minus30 = unixstamp - 2629743\n",
        "#     ccnum = cc_num\n",
        "#     sumtable = df.loc[(df[\"cc_num\"] == ccnum) & (df['unix_time'] < unixstamp) & (df['unix_time'] > minus30)]\n",
        "#     history30 = sumtable['amt'].sum()\n",
        "#     return history30\n",
        "\n",
        "\n",
        "# # running function and creating a new variable for it\n",
        "# df['history_30'] = df.apply(lambda x: sum_30_day(x.unix_time, x.cc_num), axis=1)\n",
        "\n",
        "\n",
        "# # measuring interaction effect with amt in new variable\n",
        "# df['interaction_30'] = df['history_30'] / df['amt']"
      ],
      "id": "ZdbzkXNkvkm8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92e90fb9"
      },
      "outputs": [],
      "source": [
        "# calculate the age of customer\n",
        "df['dob'] = pd.to_datetime(df['dob'])\n",
        "df['age'] = ((df['transdatetranstime'] - df['dob']) / np.timedelta64(1, 'Y')).round(1)\n",
        "df.age"
      ],
      "id": "92e90fb9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2ncI6w-rvB_"
      },
      "outputs": [],
      "source": [
        "# Bin the age feature using cut function\n",
        "age_bins = [0, 30, 45, 60, 75, np.inf]\n",
        "age_labels = ['< 30', '30-45', '46-60', '61-75', '> 75']\n",
        "df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "df['age_group'].head()"
      ],
      "id": "m2ncI6w-rvB_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyUciHrtC0Pn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib.ticker as ticker  # Import ticker module for formatting\n",
        "\n",
        "def calculate_ratio(df, bin_edges, column):\n",
        "    # Create a new column 'group' based on binning the specified column\n",
        "    df['group'] = pd.cut(df[column], bins=bin_edges)\n",
        "\n",
        "    # Group the DataFrame by 'group' and calculate normalized counts of fraud and non-fraud transactions\n",
        "    grouped = df.groupby('group')['isfraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "    # Normalize both fraud and non-fraud counts\n",
        "    grouped[0] /= grouped[0].sum()\n",
        "    grouped[1] /= grouped[1].sum()\n",
        "\n",
        "    # Calculate the normalized fraud-to-non-fraud ratio for each age group\n",
        "    grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "    # Calculate the normalized non-fraud-to-fraud ratio for each age group\n",
        "    grouped['non_fraud_ratio'] = grouped[0] / grouped[1]\n",
        "\n",
        "    return grouped\n",
        "\n",
        "# Define the bin edges for age groups\n",
        "age_bin_edges = [0, 30, 45, 60, 75, np.inf]\n",
        "bin_labels = ['< 30', '30 - 44', '45 - 59', '60 - 74', '75+']\n",
        "\n",
        "# Calculate the normalized fraud-to-non-fraud and non-fraud-to-fraud ratios for age groups\n",
        "age_ratio_df = calculate_ratio(df, age_bin_edges, 'age')\n",
        "\n",
        "# Convert the age group intervals to strings\n",
        "age_ratio_df['age_group'] = age_ratio_df['group'].astype(str)\n",
        "\n",
        "# Plot both normalized fraud and non-fraud ratios on the same bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "fraud_bars = plt.barh(age_ratio_df['age_group'], age_ratio_df['fraud_ratio'], label='Fraud Ratio', color='blue')\n",
        "non_fraud_bars = plt.barh(age_ratio_df['age_group'], -age_ratio_df['non_fraud_ratio'], label='Non-Fraud Ratio', alpha=0.7, color='orange')  # Use negative values for non-fraud ratio\n",
        "\n",
        "for bar in fraud_bars:\n",
        "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}', va='center', color='black', fontsize=10)\n",
        "\n",
        "for bar in non_fraud_bars:\n",
        "    plt.text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()/2, f'{-bar.get_width():.2f}', va='center', ha='right', color='black', fontsize=10)\n",
        "\n",
        "plt.xlabel('Ratio')\n",
        "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(-x) if x < 0 else str(x)))  # Set x-axis formatter\n",
        "plt.ylabel('Age Group')\n",
        "plt.title('Fraud and Non-Fraud Ratios by Age Group')\n",
        "plt.legend(loc='upper left')  # Adjust the legend position\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "eyUciHrtC0Pn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnkeNvGYbbWI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "ax = sns.histplot(data = df, x = 'trans_hour', hue= 'isfraud', common_norm = False, stat = 'percent', multiple = 'dodge')\n",
        "ax.set_xlabel('Time(Hour) in a Day')\n",
        "ax.set_ylabel('Percentage')\n",
        "plt.xticks(np.arange(0,24,1))\n",
        "plt.legend(title = 'Type', labels = ['Fraud','Non-Fraud'])\n",
        "plt.show()"
      ],
      "id": "QnkeNvGYbbWI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgI9udNvJYMC"
      },
      "outputs": [],
      "source": [
        "# Define a function to map hour values to time intervals\n",
        "def map_time_of_day(hour):\n",
        "    if 4 <= hour < 7:\n",
        "        return 'Early Morning'\n",
        "    elif 7 <= hour < 11:\n",
        "        return 'Morning'\n",
        "    elif 11 <= hour < 16:\n",
        "        return 'Afternoon'\n",
        "    elif 16 <= hour < 22:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "# Apply the mapping function to create the 'time_of_day' column\n",
        "df['time_of_day'] = df['trans_hour'].apply(map_time_of_day)\n",
        "\n",
        "# Plot the histogram with the new 'time_of_day' column\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = sns.histplot(data=df, x='time_of_day', hue='isfraud', common_norm=False, stat='percent', multiple='dodge')\n",
        "ax.set_xlabel('Time of Day')\n",
        "ax.set_ylabel('Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Type', labels=['Fraud', 'Non-Fraud'])\n",
        "plt.show()\n",
        "\n",
        "df.time_of_day"
      ],
      "id": "ZgI9udNvJYMC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtTec6GycHVd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_ratio(df):\n",
        "    # Group the DataFrame by 'time_of_day' and calculate normalized counts of fraud and non-fraud transactions\n",
        "    grouped = df.groupby('time_of_day')['isfraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "    # Normalize both fraud and non-fraud counts for time of day\n",
        "    grouped[0] /= grouped[0].sum()\n",
        "    grouped[1] /= grouped[1].sum()\n",
        "\n",
        "    # Calculate the normalized non-fraud-to-fraud ratio for each time category\n",
        "    grouped['non_fraud_ratio'] = grouped[0] / grouped[1]\n",
        "\n",
        "    # Calculate the normalized fraud-to-non-fraud ratio for each time category\n",
        "    grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "\n",
        "    return grouped\n",
        "\n",
        "# Calculate the normalized non-fraud-to-fraud and fraud-to-non-fraud ratios for time of day\n",
        "ratio_df = calculate_ratio(df)\n",
        "\n",
        "# Plot both the non-fraud-to-fraud ratio and fraud-to-non-fraud ratio by time of day in a rotated graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "fraud_bars = plt.barh(ratio_df['time_of_day'], ratio_df['fraud_ratio'], label='Fraud Ratio', color='blue')\n",
        "non_fraud_bars = plt.barh(ratio_df['time_of_day'], -ratio_df['non_fraud_ratio'], label='Non-Fraud Ratio', alpha=0.7, color='orange')  # Use negative values for non-fraud ratio\n",
        "plt.xlabel('Ratio')\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(-x) if x < 0 else str(x)))\n",
        "plt.ylabel('Time of Day')\n",
        "plt.title('Fraud and Non-fraud Ratios by Time of Day')\n",
        "\n",
        "# Display the ratios on the right side of each bar\n",
        "for bar in fraud_bars:\n",
        "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}', va='center', color='black', fontsize=10)\n",
        "\n",
        "for bar in non_fraud_bars:\n",
        "    plt.text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()/2, f'{-bar.get_width():.2f}', va='center', ha='right', color='black', fontsize=10)\n",
        "\n",
        "plt.yticks(rotation=45)  # Rotate y-axis labels\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "GtTec6GycHVd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kv6kbWwh9Xi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "ax = sns.histplot(data= df, x='gender', hue='isfraud', stat='percent', common_norm = False, multiple = 'dodge')\n",
        "ax.set_xlabel('Credit Card Holder Gender')\n",
        "ax.set_ylabel('Percentage')\n",
        "plt.legend(title = 'Type', labels= ['Fraud','Not-Fraud'])\n",
        "plt.show()"
      ],
      "id": "8Kv6kbWwh9Xi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taIu0ajkqj-j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_ratio(df):\n",
        "    # Group the DataFrame by 'gender' and calculate the normalized counts of fraud and non-fraud transactions\n",
        "    grouped = df.groupby('gender')['isfraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "    # Normalize both fraud and non-fraud counts for gender\n",
        "    grouped[0] /= grouped[0].sum()\n",
        "    grouped[1] /= grouped[1].sum()\n",
        "\n",
        "    # Calculate the normalized non-fraud-to-fraud ratio for each gender\n",
        "    grouped['non_fraud_ratio'] = grouped[0] / grouped[1]\n",
        "\n",
        "    # Calculate the normalized fraud-to-non-fraud ratio for each gender\n",
        "    grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "\n",
        "    return grouped\n",
        "\n",
        "# Calculate the normalized non-fraud-to-fraud and fraud-to-non-fraud ratios for gender\n",
        "ratio_df = calculate_ratio(df)\n",
        "\n",
        "# Plot both the non-fraud-to-fraud ratio and fraud-to-non-fraud ratio by gender in opposite directions\n",
        "plt.figure(figsize=(10, 5))\n",
        "fraud_bars = plt.barh(ratio_df['gender'], ratio_df['fraud_ratio'], label='Fraud Ratio', color='blue')\n",
        "non_fraud_bars = plt.barh(ratio_df['gender'], -ratio_df['non_fraud_ratio'], label='Non-Fraud Ratio', alpha=0.7, color='orange')  # Use negative values for non-fraud ratio\n",
        "plt.ylabel('Gender')\n",
        "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(-x) if x < 0 else str(x)))\n",
        "plt.xlabel('Ratio')\n",
        "plt.title('Fraud and Non-fraud Ratios by Gender')\n",
        "plt.legend()\n",
        "\n",
        "# Display the ratio values on the bars\n",
        "for bar in fraud_bars:\n",
        "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}', va='center', color='black', fontsize=10)\n",
        "\n",
        "for bar in non_fraud_bars:\n",
        "    plt.text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()/2, f'{-bar.get_width():.2f}', va='center', ha='right', color='black', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "id": "taIu0ajkqj-j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewHiHF5kI6R2"
      },
      "outputs": [],
      "source": [
        "# Calculate the value counts of each category\n",
        "category_counts = df['category'].value_counts(normalize=True)\n",
        "\n",
        "# Plot the count plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plot = sns.countplot(x='category', data=df, hue='isfraud')\n",
        "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Transaction Count by Category')\n",
        "plt.legend(title='Fraud', labels=['Non-Fraud', 'Fraud'])\n",
        "plt.show()"
      ],
      "id": "ewHiHF5kI6R2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjJJiAqLKEd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_ratio(df, column):\n",
        "    # Group the DataFrame by the specified column and calculate normalized counts of fraud and non-fraud transactions\n",
        "    grouped = df.groupby(column)['isfraud'].value_counts(normalize=True).unstack().reset_index()\n",
        "\n",
        "    # Normalize both fraud and non-fraud counts\n",
        "    grouped[0] /= grouped[0].sum()\n",
        "    grouped[1] /= grouped[1].sum()\n",
        "\n",
        "    # Calculate the normalized fraud-to-non-fraud ratio for each group\n",
        "    grouped['fraud_ratio'] = grouped[1] / grouped[0]\n",
        "    # Calculate the normalized non-fraud-to-fraud ratio for each group\n",
        "    grouped['non_fraud_ratio'] = grouped[0] / grouped[1]\n",
        "\n",
        "    return grouped\n",
        "\n",
        "# Calculate the normalized fraud-to-non-fraud and non-fraud-to-fraud ratios for merchant categories\n",
        "category_ratio_df = calculate_ratio(df, 'category')\n",
        "\n",
        "# Sort the DataFrame by 'category' in ascending order\n",
        "category_ratio_df_sorted = category_ratio_df.sort_values('category')\n",
        "\n",
        "# Plot both normalized fraud and non-fraud ratios on the same bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "fraud_bars = plt.barh(category_ratio_df_sorted['category'], category_ratio_df_sorted['fraud_ratio'], label='Fraud Ratio', color='blue')\n",
        "non_fraud_bars = plt.barh(category_ratio_df_sorted['category'], -category_ratio_df_sorted['non_fraud_ratio'], label='Non-Fraud Ratio', alpha=0.7, color='orange')  # Use negative values for non-fraud ratio\n",
        "\n",
        "for bar in fraud_bars:\n",
        "    plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}', va='center', color='black', fontsize=10)\n",
        "\n",
        "for bar in non_fraud_bars:\n",
        "    plt.text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()/2, f'{-bar.get_width():.2f}', va='center', ha='right', color='black', fontsize=10)\n",
        "\n",
        "plt.xlabel('Ratio')\n",
        "plt.ylabel('Category')\n",
        "plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(-x) if x < 0 else str(x)))\n",
        "plt.title('Fraud and Non-Fraud Ratios by Category')\n",
        "plt.legend(loc='lower right')  # Adjust the legend position\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "LjJJiAqLKEd5"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "c283f45f"
      },
      "outputs": [],
      "source": [
        "# remove first name, last name, data of birth and transaction date/time\n",
        "df.drop(['first','last','dob','transdatetranstime','transdate'], axis = 1, inplace= True)"
      ],
      "id": "c283f45f"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "34920a19"
      },
      "outputs": [],
      "source": [
        "# storing a copy\n",
        "df_org = df.copy()"
      ],
      "id": "34920a19"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a17107f"
      },
      "outputs": [],
      "source": [
        "100*df.isfraud.value_counts(normalize=True)"
      ],
      "id": "6a17107f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4Becvu-9mBT"
      },
      "outputs": [],
      "source": [
        "# df_sorted.nunique()"
      ],
      "id": "i4Becvu-9mBT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGQCXi6paT3b"
      },
      "outputs": [],
      "source": [
        "# df2['distance'] = np.sqrt((df['lat']-df['merch_lat'])**2 + (df['long']-df['merch_long'])**2)"
      ],
      "id": "mGQCXi6paT3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsr5RvwPqLx8"
      },
      "outputs": [],
      "source": [
        "# df = df_sorted.copy()\n",
        "# df.info()"
      ],
      "id": "Xsr5RvwPqLx8"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "26cc2e2f"
      },
      "outputs": [],
      "source": [
        "category_onehot = pd.get_dummies(df.category, prefix = 'category',drop_first = True)\n",
        "gender_onehot = pd.get_dummies(df.gender, prefix = 'gender', drop_first = True)\n",
        "day_of_week_onehot = pd.get_dummies(df.trans_day_of_week, prefix = 'day', drop_first = True)\n",
        "time_of_day_onehot = pd.get_dummies(df.time_of_day, prefix = 'time_of_day', drop_first = True)\n",
        "#trans_year_month_onehot = pd.get_dummies(df.trans_year_month, prefix = 'year', drop_first = True)\n",
        "age_group_onehot = pd.get_dummies(df.age_group , prefix = 'age', drop_first = True)"
      ],
      "id": "26cc2e2f"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "gmF-q-eKpewj"
      },
      "outputs": [],
      "source": [
        "df2 = pd.concat([df, category_onehot,gender_onehot,day_of_week_onehot,time_of_day_onehot,age_group_onehot],axis = 1)"
      ],
      "id": "gmF-q-eKpewj"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "b-kBmXuqpozs"
      },
      "outputs": [],
      "source": [
        "df2.drop(['ccnum', 'recnum','transnum','amount_bin','merchant','street','city','state','job','category','gender','trans_day_of_week','age','lat','long','citypop','unixtime','merchlat','merchlong','trans_hour','age_group','group','time_of_day'],axis = 1, inplace = True)\n"
      ],
      "id": "b-kBmXuqpozs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1U987lep7LO"
      },
      "outputs": [],
      "source": [
        "print(df2.shape)\n",
        "df2.columns"
      ],
      "id": "g1U987lep7LO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs2L7QB8sFNw"
      },
      "outputs": [],
      "source": [
        "# df1.drop(['lat','long','city_pop','merch_lat','merch_long'],axis = 1, inplace = True)\n",
        "# print(df1.columns)"
      ],
      "id": "vs2L7QB8sFNw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW6ceXdAv6dE"
      },
      "outputs": [],
      "source": [
        "# fig, ax = plt.subplots(figsize=(20,10))\n",
        "# sns.heatmap(df1.corr(),cmap = 'coolwarm').set_title('Correlation')"
      ],
      "id": "GW6ceXdAv6dE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99p6wTUfwH9V"
      },
      "outputs": [],
      "source": [
        "# fig = plt.figure(figsize = (18,9))\n",
        "# sns.heatmap(df2.corr(), cmap = 'coolwarm', annot = False)\n",
        "# plt.show()"
      ],
      "id": "99p6wTUfwH9V"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zaVEheSYmCNB"
      },
      "outputs": [],
      "source": [
        "non_fraud_count, fraud_count = df2.isfraud.value_counts()"
      ],
      "id": "zaVEheSYmCNB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wke90jYroBoc"
      },
      "outputs": [],
      "source": [
        "df_num = df2.select_dtypes(include = 'number')\n",
        "#df_num.drop(['index'], axis = 1, inplace = True)\n",
        "df_num.columns"
      ],
      "id": "wke90jYroBoc"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "uqScBl94TI65"
      },
      "outputs": [],
      "source": [
        "# Data Standardiztion\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "rob_scaler = RobustScaler()\n",
        "# Scales all variables\n",
        "df_scaled = rob_scaler.fit_transform(df_num)\n",
        "df_num = pd.DataFrame(df_scaled, columns=df_num.columns)"
      ],
      "id": "uqScBl94TI65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk0JNlWpmXAi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "print('No Frauds', round(df_num['isfraud'].value_counts()[0] / len(df_num) * 100, 2), '% of the dataset')\n",
        "print('Frauds', round(df_num['isfraud'].value_counts()[1] / len(df_num) * 100, 2), '% of the dataset')\n",
        "\n",
        "X = df_num.drop('isfraud', axis=1)\n",
        "y = df_num['isfraud']\n",
        "\n",
        "sss = StratifiedKFold(n_splits=5, random_state= None , shuffle= True)\n",
        "\n",
        "for train_index, test_index in sss.split(X, y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
        "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "\n",
        "# Check the Distribution of the labels\n",
        "\n",
        "# Turn into arrays\n",
        "original_Xtrain = original_Xtrain.values\n",
        "original_Xtest = original_Xtest.values\n",
        "original_ytrain = original_ytrain.values\n",
        "original_ytest = original_ytest.values\n",
        "\n",
        "# See if both the train and test label distribution are similarly distributed\n",
        "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
        "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
        "\n"
      ],
      "id": "Xk0JNlWpmXAi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q-17Vukmlzu"
      },
      "outputs": [],
      "source": [
        "# Since our classes are highly skewed, we should make them equivalent in order to have a balanced distribution of the classes.\n",
        "\n",
        "# Let's shuffle the data before creating the subsamples\n",
        "df_num = df_num.sample(frac=1)\n",
        "\n",
        "# Amount of fraud cases (453 rows).\n",
        "fraud_df = df_num.loc[df_num['isfraud'] == 1]\n",
        "non_fraud_df = df_num.loc[df_num['isfraud'] == 0][:453]  # Update the number to match your fraud cases count\n",
        "\n",
        "balanced_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = balanced_df.sample(frac=1, random_state=42)\n",
        "\n",
        "new_df.head()\n"
      ],
      "id": "0Q-17Vukmlzu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDsaW7rMraTU"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Distribution of the Classes in the subsample dataset')\n",
        "print(new_df['isfraud'].value_counts() / len(new_df))\n",
        "\n",
        "colors = [\"#0101DF\", \"#DF0101\"]  # Blue for non-fraud, Red for fraud\n",
        "\n",
        "plt.figure(figsize=(7, 4))  # Adjust the figure size if needed\n",
        "sns.countplot(x='isfraud', data=new_df, palette=colors)\n",
        "plt.title('Equally Distributed Classes', fontsize=14)\n",
        "plt.show()\n"
      ],
      "id": "ZDsaW7rMraTU"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Q5X94rEKeI8S"
      },
      "outputs": [],
      "source": [
        "## Feature Selection using LASSO ##\n",
        "X_cols = new_df.columns.drop('isfraud')\n",
        "Y_cols = 'isfraud'"
      ],
      "id": "Q5X94rEKeI8S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egCYK0jjeNch"
      },
      "outputs": [],
      "source": [
        "# Implementing LASSO regression onto dataset\n",
        "from sklearn.linear_model import LassoCV\n",
        "X = pd.DataFrame(new_df[X_cols], columns=X_cols) # feature matrix\n",
        "y = new_df[Y_cols] # target variable\n",
        "\n",
        "reg = LassoCV()\n",
        "reg.fit(X,y)\n",
        "print(\"Best alpha using built_in LassoCV = %f\" %reg.alpha_)\n",
        "print(\"Best score using built-in LassoCV = %f\" %reg.score(X,y))"
      ],
      "id": "egCYK0jjeNch"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS8LujlVeeIv"
      },
      "outputs": [],
      "source": [
        "coef = pd.Series(reg.coef_, index=X.columns)\n",
        "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" + str(sum(coef == 0)) + \" variables\")\n",
        "imp_coef = coef[coef != 0].sort_values()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8.0, 10.0))\n",
        "imp_coef.plot(kind=\"barh\")\n",
        "plt.title(\"Important Variables Selected by Lasso model\")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.ylabel(\"Variable\")\n",
        "plt.show()\n",
        "\n",
        "picked_variables = X.columns[coef != 0]\n",
        "print(\"Variables picked by Lasso:\", picked_variables)\n"
      ],
      "id": "iS8LujlVeeIv"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "JlNvBZVree_J"
      },
      "outputs": [],
      "source": [
        "selected_columns = ['ccnum_freq3_transdate', 'ccnum_freq30_transdate',\n",
        "       'ccnum_0_dayfreq_div_14_dayfreq_velchange',\n",
        "       'ccnum_0_dayfreq_div_30_dayfreq_velchange',\n",
        "       'merchant_0_dayfreq_div_30_dayfreq_velchange',\n",
        "       'card-merchant_daysSince', 'ccnum_totalamount1_amt2',\n",
        "       'merchant_totalamount7_amt2', 'merchant_totalamount14_amt2',\n",
        "       'merchant_totalamount30_amt2', 'ccnum_median0_amt2',\n",
        "       'ccnum_median1_amt2', 'ccnum_median30_amt2', 'merchant_median1_amt2',\n",
        "       'merchant_median7_amt2', 'merchant_median30_amt2',\n",
        "       'merchant_mean0_amt2', 'merchant_mean1_amt2', 'ccnum_max1_amt2',\n",
        "       'ccnum_max3_amt2', 'ccnum_max30_amt2', 'merchant_max30_amt2',\n",
        "       'ccnum_median_1_actual', 'ccnum_median_3_actual',\n",
        "       'ccnum_median_7_actual', 'ccnum_median_14_actual',\n",
        "       'ccnum_median_30_actual', 'merchant_median_3_actual',\n",
        "       'merchant_median_7_actual', 'merchant_median_14_actual',\n",
        "       'merchant_median_30_actual', 'ccnum_mean_30_actual',\n",
        "       'merchant_mean_3_actual', 'merchant_mean_30_actual',\n",
        "       'ccnum_max_14_actual', 'merchant_max_3_actual', 'merchant_max_7_actual',\n",
        "       'category_grocery_pos', 'category_shopping_net', 'day_Monday',\n",
        "       'day_Wednesday', 'time_of_day_Night', 'age_61-75', 'isfraud']\n",
        "\n",
        "df_filtered = df_num[selected_columns]"
      ],
      "id": "JlNvBZVree_J"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "E6j6lcdXNI4J"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression - imbalanced data\n",
        "X = df_filtered.drop('isfraud', axis=1)\n",
        "y = df_filtered['isfraud']"
      ],
      "id": "E6j6lcdXNI4J"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Ne_F3dpyPY2r"
      },
      "outputs": [],
      "source": [
        "# Our data is already scaled we should split our training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting Train-Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "id": "Ne_F3dpyPY2r"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "log_reg_params = {'C': [0.001, 0.01, 0.1, 1.0], 'max_iter': [2000], 'random_state': [13]}\n",
        "\n",
        "# Create a cross-validation strategy (Stratified K-Fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "# Initialize GridSearchCV with Logistic Regression, hyperparameter grid, and cross-validation\n",
        "grid_log_reg = GridSearchCV(LogisticRegression(random_state=13), log_reg_params, cv=cv, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to your training data\n",
        "grid_log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "teNMUuKOT6mJ"
      },
      "id": "teNMUuKOT6mJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
      ],
      "metadata": {
        "id": "yEDbKIlJfV9x"
      },
      "id": "yEDbKIlJfV9x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = XGBClassifier()"
      ],
      "metadata": {
        "id": "_AVwjChU4WSq"
      },
      "id": "_AVwjChU4WSq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "classifier.fit(X_train, y_train)\n",
        "timer(start_time) # timing ends here for \"start_time\" variable"
      ],
      "metadata": {
        "id": "ZYYHFf2-yaC3"
      },
      "id": "ZYYHFf2-yaC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(random_state = 13)\n",
        "\n",
        "params = {\n",
        "    \"learning_rate\": [0.1,1,3],\n",
        "    \"max_depth\":[6,7,8],\n",
        "    \"subsample\": [0.5, 0.7, 1.0],\n",
        "    \"colsample_bytree\": [0.5, 0.8, 1.0],\n",
        "    \"n_estimators\" : [500,600,1000]\n",
        "    }\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "xgb_grid = GridSearchCV(estimator=xgb, param_grid=params, scoring=\"recall\", cv=cv)\n",
        "xgb_grid.fit(X, y)"
      ],
      "metadata": {
        "id": "DjeGPSjSlZoD"
      },
      "id": "DjeGPSjSlZoD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictionXGB = xgb.predict(X_test)\n",
        "print(confusion_matrix(y_test,predictionXGB))\n",
        "print(classification_report(y_test,predictionXGB))"
      ],
      "metadata": {
        "id": "ebqv0TbeyjZE"
      },
      "id": "ebqv0TbeyjZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Optuna for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Fit the model\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "kL64v_vZyjl6"
      },
      "id": "kL64v_vZyjl6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install optuna"
      ],
      "metadata": {
        "id": "59F4qQsUoEiP"
      },
      "id": "59F4qQsUoEiP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import optuna"
      ],
      "metadata": {
        "id": "kGKb9_WAoLub"
      },
      "id": "kGKb9_WAoLub",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='maximize')"
      ],
      "metadata": {
        "id": "om3_e1Bbnm23"
      },
      "id": "om3_e1Bbnm23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "id": "-8ivgnDcoU08"
      },
      "id": "-8ivgnDcoU08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "_w2ni61IzFFV"
      },
      "id": "_w2ni61IzFFV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of finished trials: {}'.format(len(study.trials)))\n",
        "print('Best trial:')\n",
        "\n",
        "print('  Value: {:.2f}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {:.3f}'.format(key, value))"
      ],
      "metadata": {
        "id": "IxYVsZxQ9hxM"
      },
      "id": "IxYVsZxQ9hxM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = trial.params"
      ],
      "metadata": {
        "id": "3IuV1KwvywmD"
      },
      "id": "3IuV1KwvywmD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(**params)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "K0LcM8Yuywvj"
      },
      "id": "K0LcM8Yuywvj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "hev61XRLzPLK"
      },
      "id": "hev61XRLzPLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "gr_CEBMlzRB0"
      },
      "id": "gr_CEBMlzRB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "xgb_matrix = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(xgb_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "ESxiWctv_81J"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ESxiWctv_81J"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"XGBoost, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F80PWH-H_81J"
      },
      "execution_count": null,
      "outputs": [],
      "id": "F80PWH-H_81J"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    model, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "iP_NCd9UzaHf"
      },
      "id": "iP_NCd9UzaHf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGboost Random Undersampling\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Apply Random Undersampling to the training data\n",
        "    sampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_train_rus, y_train_rus = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Fit the model on the resampled data\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train_rus, y_train_rus)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "6LTe2oIilZ6Z"
      },
      "id": "6LTe2oIilZ6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ],
      "metadata": {
        "id": "qQpblYXzEatd"
      },
      "id": "qQpblYXzEatd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "BkCGMVrzK5fk"
      },
      "id": "BkCGMVrzK5fk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_rus = trial.params"
      ],
      "metadata": {
        "id": "tuuu4m2EGFi-"
      },
      "id": "tuuu4m2EGFi-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_under = XGBClassifier(**params_rus)\n",
        "XGB_under.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Liq2YZeeGQGh"
      },
      "id": "Liq2YZeeGQGh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = XGB_under.predict(X_test)"
      ],
      "metadata": {
        "id": "m52rzGjtGfYU"
      },
      "id": "m52rzGjtGfYU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "fX5t_o8-Gfgb"
      },
      "id": "fX5t_o8-Gfgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_xgb_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_xgb_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "2XtZE_Y1Gp1j"
      },
      "id": "2XtZE_Y1Gp1j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = XGB_under.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"XGBoost, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bjv0UcC-HD3L"
      },
      "id": "bjv0UcC-HD3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    XGB_under, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "T0N_k_YGHSRG"
      },
      "id": "T0N_k_YGHSRG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGboost Random Oversampling\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Apply Random Oversampling to the training data\n",
        "    sampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_train_ros, y_train_ros = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Fit the model on the resampled data\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train_ros, y_train_ros)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "v9W7lRdJHD_z"
      },
      "id": "v9W7lRdJHD_z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "best_params = study.best_params\n",
        "formatted_params = {key: f\"{value:.3f}\" for key, value in best_params.items()}\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key, value in formatted_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4vFiw1TLPJ6d"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4vFiw1TLPJ6d"
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "uXCapNc-PJ6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uXCapNc-PJ6e"
    },
    {
      "cell_type": "code",
      "source": [
        "params_ros = trial.params"
      ],
      "metadata": {
        "id": "0agVPgJcPJ6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "0agVPgJcPJ6e"
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_over = XGBClassifier(**params_ros)\n",
        "XGB_over.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "R9l4EhSCPJ6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "R9l4EhSCPJ6e"
    },
    {
      "cell_type": "code",
      "source": [
        "predict_xgb_over = XGB_over.predict(X_test)\n",
        "print(confusion_matrix(y_test,predict_xgb_over))\n",
        "print(classification_report(y_test,predict_xgb_over))"
      ],
      "metadata": {
        "id": "ayQvtjOcipJV"
      },
      "id": "ayQvtjOcipJV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,predict_xgb_over)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "nUd9RjyzjWij"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nUd9RjyzjWij"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = XGB_over.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7_UZlWpkjWis"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7_UZlWpkjWis"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    XGB_over, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "Q6ob559ojWis"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Q6ob559ojWis"
    },
    {
      "cell_type": "code",
      "source": [
        "# XGboost SMOTE\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 6, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 5, 15),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.1, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.6, 1.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01,1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 600,1000,100),\n",
        "        'eval_metric': 'aucpr'\n",
        "    }\n",
        "\n",
        "    # Apply Random Oversampling to the training data\n",
        "    sampler = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "    X_train_smote, y_train_smote = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Fit the model on the resampled data\n",
        "    optuna_model = XGBClassifier(**params)\n",
        "    optuna_model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(X_test)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    return recall"
      ],
      "metadata": {
        "id": "urToH-jxipW2"
      },
      "id": "urToH-jxipW2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "best_params = study.best_params\n",
        "formatted_params = {key: f\"{value:.3f}\" for key, value in best_params.items()}\n",
        "\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key, value in formatted_params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "id": "VfZ7A2A0ipc7"
      },
      "id": "VfZ7A2A0ipc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trial = study.best_trial"
      ],
      "metadata": {
        "id": "JP-m5QDeC_DT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JP-m5QDeC_DT"
    },
    {
      "cell_type": "code",
      "source": [
        "params_smote = trial.params"
      ],
      "metadata": {
        "id": "vhEwgnNjC_Db"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vhEwgnNjC_Db"
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_smote = XGBClassifier(**params_smote)\n",
        "XGB_smote.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "X3JC9XN8C_Db"
      },
      "execution_count": null,
      "outputs": [],
      "id": "X3JC9XN8C_Db"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = XGB_smote.predict(X_test)"
      ],
      "metadata": {
        "id": "l_VXmL-WC_Db"
      },
      "execution_count": null,
      "outputs": [],
      "id": "l_VXmL-WC_Db"
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "yxLVvfsFC_Db"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yxLVvfsFC_Db"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_xgb_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_xgb_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "q1pPc51TC_Db"
      },
      "execution_count": null,
      "outputs": [],
      "id": "q1pPc51TC_Db"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = XGB_smote.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"XGBoost, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hf2MLei8C_Dc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hf2MLei8C_Dc"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    XGB_smote, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "bRPbH2HwC_Dc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bRPbH2HwC_Dc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAIQwtb5mRbZ"
      },
      "id": "HAIQwtb5mRbZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQt9PnLZPY7U"
      },
      "outputs": [],
      "source": [
        "# Turn the values into an array for feeding the classification algorithms.\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ],
      "id": "nQt9PnLZPY7U"
    },
    {
      "cell_type": "code",
      "source": [
        "# lr = LogisticRegression(C = 1.0, random_state=42, max_iter = 1500)\n",
        "# model = lr.fit(X_train, y_train)\n",
        "# y_train_pred = model.predict(X_train)\n",
        "# print('y_train_pred: ',y_train_pred)\n",
        "# y_test_pred = model.predict(X_test)\n",
        "# print('y_test_pred: ', y_test_pred)"
      ],
      "metadata": {
        "id": "J-j7Wr5DzEqx"
      },
      "id": "J-j7Wr5DzEqx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #evaluating the model\n",
        "# model_name = 'Logistic Regression - without balancing'\n",
        "# train_score = model.score(X_train,y_train)\n",
        "# test_score = model.score(X_test,y_test)\n",
        "\n",
        "# acc_score = accuracy_score(y_test,y_test_pred)\n",
        "# f_score = f1_score(y_test, y_test_pred)\n",
        "# precision = precision_score(y_test, y_test_pred)\n",
        "# recall = metrics.recall_score(y_test,y_test_pred)\n",
        "# #creating a dataframe to compare the performance of different models\n",
        "# model_eval_data = [[model_name, train_score, test_score, acc_score, f_score, precision, recall]]\n",
        "# evaluate_df = pd.DataFrame(model_eval_data, columns=['Model Name', 'Training Score', 'Testing Score', 'Accuracy',\n",
        "#                                           'F1 Score', 'Precision', 'Recall'])\n",
        "# evaluate_df"
      ],
      "metadata": {
        "id": "JLHkZYuDzMYi"
      },
      "id": "JLHkZYuDzMYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# report = classification_report(y_test, y_test_pred)\n",
        "\n",
        "# # Print the classification report\n",
        "# print(report)"
      ],
      "metadata": {
        "id": "hgT9OuvCzzOX"
      },
      "id": "hgT9OuvCzzOX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# # Logistic Regression\n",
        "# log_reg_params = {'C': [0.001, 0.01, 0.1, 1.0],'max_iter' : [2000]}\n",
        "\n",
        "\n",
        "# grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
        "# grid_log_reg.fit(X_train, y_train)\n",
        "# # We automatically get the logistic regression with the best parameters.\n",
        "# log_reg = grid_log_reg.best_estimator_\n",
        "\n",
        "# print(\"Best C value for Logistic Regression: \", grid_log_reg.best_params_['C'])"
      ],
      "metadata": {
        "id": "yJfcNf941n3F"
      },
      "id": "yJfcNf941n3F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "log_reg_params = { 'C': [0.001, 0.01, 0.1, 1.0], 'max_iter' :[2000], 'random_state':[13]}\n",
        "\n",
        "# Create a cross-validation strategy (Stratified K-Fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "# Initialize GridSearchCV with Logistic Regression, hyperparameter grid, and cross-validation\n",
        "grid_log_reg = GridSearchCV(LogisticRegression(random_state = 13), log_reg_params, cv=cv, scoring = 'recall')\n",
        "\n",
        "# Fit the grid search to your training data\n",
        "grid_log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters for Logistic Regression:\", grid_log_reg.best_params_['C'])\n",
        "\n",
        "# Get the best estimator\n",
        "best_log_reg = grid_log_reg.best_estimator_"
      ],
      "metadata": {
        "id": "X8XBILzsJdyW"
      },
      "id": "X8XBILzsJdyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictionLR = grid_log_reg.predict(X_test)\n",
        "print(confusion_matrix(y_test,predictionLR))\n",
        "print(classification_report(y_test,predictionLR))"
      ],
      "metadata": {
        "id": "AyNCxKnwOWhG"
      },
      "id": "AyNCxKnwOWhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,predictionLR)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "WOtQn4hePtZo"
      },
      "id": "WOtQn4hePtZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = best_log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pa4kFJ44QVVJ"
      },
      "id": "Pa4kFJ44QVVJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    best_log_reg, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "vF5aGY2_YjqU"
      },
      "id": "vF5aGY2_YjqU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=False)"
      ],
      "metadata": {
        "id": "lJjjX-OpD0ru"
      },
      "id": "lJjjX-OpD0ru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression - Undersampling\n",
        "rus = RandomUnderSampler(random_state = 42)\n",
        "X_rus, y_rus = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "13Q_LI3kMTD1"
      },
      "id": "13Q_LI3kMTD1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_under_pipeline = make_pipeline(RandomUnderSampler(random_state=42),\n",
        "                                        LogisticRegression(random_state=13, max_iter=1500))"
      ],
      "metadata": {
        "id": "8gn3yKEz42L2"
      },
      "id": "8gn3yKEz42L2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid, specifying 'logisticregression__' for the C parameter\n",
        "new_params = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1,1.0]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_under_logistic = GridSearchCV(\n",
        "    logistic_under_pipeline,  # Your pipeline with Logistic Regression\n",
        "    param_grid=new_params,  # The hyperparameter grid\n",
        "    cv= cv ,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_under_logistic.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "6xgDrxKX71uh"
      },
      "id": "6xgDrxKX71uh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_under_logistic.best_params_)\n",
        "print('Best score:', grid_under_logistic.best_score_)"
      ],
      "metadata": {
        "id": "aWb5XHnz_bTB"
      },
      "id": "aWb5XHnz_bTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_under_logistic.best_estimator_.named_steps['logisticregression'].predict(X_test)"
      ],
      "metadata": {
        "id": "5cjBR5eYAuMl"
      },
      "id": "5cjBR5eYAuMl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "tjV2cVCIBVOU"
      },
      "id": "tjV2cVCIBVOU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "-8_5P9Z9B7SQ"
      },
      "id": "-8_5P9Z9B7SQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_under_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f0dcQQeV48FL"
      },
      "id": "f0dcQQeV48FL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_under_logistic, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "3nftdzuf5gq2"
      },
      "id": "3nftdzuf5gq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression - Oversampling\n",
        "ros = RandomOverSampler(random_state = 42)\n",
        "X_ros, y_ros = ros.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "BEpF0WEYEJJy"
      },
      "id": "BEpF0WEYEJJy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate the unique values and their counts\n",
        "unique_values, counts = np.unique(y_ros, return_counts=True)\n",
        "\n",
        "# Print the results\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Class {value}: {count} samples')\n"
      ],
      "metadata": {
        "id": "7bRZuNe6EeiK"
      },
      "id": "7bRZuNe6EeiK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_over_pipeline = make_pipeline(RandomOverSampler(random_state=42),\n",
        "                                        LogisticRegression(random_state=13, max_iter=1500))\n",
        "\n",
        "score2 = cross_val_score(logistic_over_pipeline, X_train, y_train, scoring='recall', cv= cv)\n",
        "print(\"Cross Validation Recall Scores are: {}\".format(score2))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score2.mean()))"
      ],
      "metadata": {
        "id": "DlUCN2iyEuTU"
      },
      "id": "DlUCN2iyEuTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid, specifying 'logisticregression__' for the C parameter\n",
        "new_params = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1,1.0]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_over_logistic = GridSearchCV(\n",
        "    logistic_over_pipeline,  # Your pipeline with Logistic Regression\n",
        "    param_grid=new_params,  # The hyperparameter grid\n",
        "    cv= cv ,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_over_logistic.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "ZJZ_AjQtE6nb"
      },
      "id": "ZJZ_AjQtE6nb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_over_logistic.best_params_)\n",
        "print('Best score:', grid_over_logistic.best_score_)"
      ],
      "metadata": {
        "id": "HJhdKOe2HvgC"
      },
      "id": "HJhdKOe2HvgC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_over_logistic.best_estimator_.named_steps['logisticregression'].predict(X_test)"
      ],
      "metadata": {
        "id": "gZ-l6MIaFFfy"
      },
      "id": "gZ-l6MIaFFfy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "KNlc7JlFFFyO"
      },
      "id": "KNlc7JlFFFyO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_over = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_over), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "7bmN1z4sFecd"
      },
      "id": "7bmN1z4sFecd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_over_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I19dpShv48Z4"
      },
      "id": "I19dpShv48Z4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_over_logistic, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "S4DDZ6pq48iO"
      },
      "id": "S4DDZ6pq48iO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Logistic Regression - SMOTE #\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_pipeline = make_pipeline(SMOTE(random_state=42),\n",
        "                              LogisticRegression(max_iter = 2000, random_state=13))\n",
        "score2 = cross_val_score(smote_pipeline, X_train, y_train, cv= cv)\n",
        "print(\"Cross Validation Recall Scores are: {}\".format(score2))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score2.mean()))"
      ],
      "metadata": {
        "id": "-dVVfxImGKAV"
      },
      "id": "-dVVfxImGKAV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameter grid, specifying 'logisticregression__' for the C parameter\n",
        "new_params = {\n",
        "    'logisticregression__C': [0.001, 0.01, 0.1,1.0]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_smote_logistic = GridSearchCV(\n",
        "    smote_pipeline,  # Your pipeline with Logistic Regression\n",
        "    param_grid=new_params,  # The hyperparameter grid\n",
        "    cv= cv ,  # Your cross-validation strategy (StratifiedKFold)\n",
        "      # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_smote_logistic.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "fQ-j-h_tM4ma"
      },
      "id": "fQ-j-h_tM4ma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_smote_logistic.best_params_)\n",
        "print('Best score:', grid_smote_logistic.best_score_)"
      ],
      "metadata": {
        "id": "piFt1T-6M43W"
      },
      "id": "piFt1T-6M43W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_smote_logistic.best_estimator_.named_steps['logisticregression'].predict(X_test)"
      ],
      "metadata": {
        "id": "hWungTxtM5AZ"
      },
      "id": "hWungTxtM5AZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "3rfAy7vKM5Kd"
      },
      "id": "3rfAy7vKM5Kd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "matrix_smote = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(matrix_smote), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "nHUY_sjWPJ9K"
      },
      "id": "nHUY_sjWPJ9K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_smote_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Logistic Regression, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6ETnPUEqPKDz"
      },
      "id": "6ETnPUEqPKDz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_smote_logistic, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "CptqfbYwPV9F"
      },
      "id": "CptqfbYwPV9F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest - imbalanced dataset #\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=False)"
      ],
      "metadata": {
        "id": "_UHfpYz-Zt_b"
      },
      "id": "_UHfpYz-Zt_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=13)"
      ],
      "metadata": {
        "id": "6ds8kxjrq_EP"
      },
      "id": "6ds8kxjrq_EP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = cross_val_score(rf, X_train, y_train, cv=kf)\n",
        "print(\"Cross Validation Recall scores are: {}\".format(score))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score.mean()))"
      ],
      "metadata": {
        "id": "_DhCDJW9q_Nd"
      },
      "id": "_DhCDJW9q_Nd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'random_state': [13]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(rf, param_grid=params, cv=kf,\n",
        "                          scoring='recall').fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7WXTb0mYq_bf"
      },
      "id": "7WXTb0mYq_bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_rf.best_params_)\n",
        "print('Best score:', grid_rf.best_score_)"
      ],
      "metadata": {
        "id": "EmrdtB9ZSycv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EmrdtB9ZSycv"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_rf.predict(X_test)"
      ],
      "metadata": {
        "id": "QLajdZCft-Ub"
      },
      "id": "QLajdZCft-Ub",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "JP1QRsGNt-YN"
      },
      "id": "JP1QRsGNt-YN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_matrix = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "PxXnVMuMzZG8"
      },
      "id": "PxXnVMuMzZG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AxuE0GbPz3-B"
      },
      "id": "AxuE0GbPz3-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "ggZ7UfRkzZP_"
      },
      "id": "ggZ7UfRkzZP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest - Oversampling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "rf_over_pipeline = make_pipeline(RandomOverSampler(random_state=42),\n",
        "                              RandomForestClassifier(n_estimators = 100, random_state=13))"
      ],
      "metadata": {
        "id": "1InSZVkfLxXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1InSZVkfLxXF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the hyperparameter grid for Random Forest\n",
        "params = {\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__random_state': [13]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_over_rf = GridSearchCV(\n",
        "    rf_over_pipeline,  # Your pipeline with Random Forest\n",
        "    param_grid=params,  # The hyperparameter grid\n",
        "    cv=cv,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_over_rf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "U3JIpteTMlHR"
      },
      "id": "U3JIpteTMlHR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_over_rf.best_params_)\n",
        "print('Best score:', grid_over_rf.best_score_)"
      ],
      "metadata": {
        "id": "X-I1ca4UOWvB"
      },
      "id": "X-I1ca4UOWvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_over_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
      ],
      "metadata": {
        "id": "_Pc967NRPCCZ"
      },
      "id": "_Pc967NRPCCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "i7-AHM63PFQz"
      },
      "id": "i7-AHM63PFQz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_over = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_over), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "ohPOn42sPFbo"
      },
      "id": "ohPOn42sPFbo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_over_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pzQlUim2PFkL"
      },
      "id": "pzQlUim2PFkL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_over_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall Curve with Class weights\")"
      ],
      "metadata": {
        "id": "Pdo5IjvTP9tF"
      },
      "id": "Pdo5IjvTP9tF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forst Undersampling\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "rf_under_pipeline = make_pipeline(RandomUnderSampler(random_state=42),\n",
        "                              RandomForestClassifier(n_estimators=100, random_state=13))\n"
      ],
      "metadata": {
        "id": "KdBCR7MsP9yw"
      },
      "id": "KdBCR7MsP9yw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the hyperparameter grid for Random Forest\n",
        "params = {\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__random_state': [13]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_under_rf = GridSearchCV(\n",
        "    rf_under_pipeline,  # Your pipeline with Random Forest\n",
        "    param_grid=params,  # The hyperparameter grid\n",
        "    cv=cv,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_under_rf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "7a1sTz2pfhQZ"
      },
      "id": "7a1sTz2pfhQZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_under_rf.best_params_)\n",
        "print('Best score:', grid_under_rf.best_score_)"
      ],
      "metadata": {
        "id": "lOrFkfADfhgr"
      },
      "id": "lOrFkfADfhgr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_under_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
      ],
      "metadata": {
        "id": "u2CE6kTIfhpt"
      },
      "id": "u2CE6kTIfhpt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "2mVzYhbZLxXM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2mVzYhbZLxXM"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_under = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_under), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "C5f-ZqkwIeOW"
      },
      "id": "C5f-ZqkwIeOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_under_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IMC6P1gEjPKw"
      },
      "id": "IMC6P1gEjPKw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_under_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall Curve with Class weights\")"
      ],
      "metadata": {
        "id": "MyWbGAfNjPO6"
      },
      "id": "MyWbGAfNjPO6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "rf_smote_pipeline = make_pipeline(SMOTE(random_state=42),\n",
        "                              RandomForestClassifier(n_estimators=100, random_state=13))\n"
      ],
      "metadata": {
        "id": "KDvOjkRzjPUO"
      },
      "id": "KDvOjkRzjPUO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid for Random Forest\n",
        "params = {\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__random_state': [13]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance, specifying the pipeline, parameter grid, cv, scoring, etc.\n",
        "grid_smote_rf = GridSearchCV(\n",
        "    rf_smote_pipeline,  # Your pipeline with Random Forest\n",
        "    param_grid=params,  # The hyperparameter grid\n",
        "    cv=cv,  # Your cross-validation strategy (StratifiedKFold)\n",
        "    scoring='recall',  # The scoring metric you want to optimize for\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit the grid search to your data\n",
        "grid_smote_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-5IuLLEuIeXP"
      },
      "id": "-5IuLLEuIeXP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_smote_rf.best_params_)\n",
        "print('Best score:', grid_smote_rf.best_score_)"
      ],
      "metadata": {
        "id": "lfw-gS03l5F4"
      },
      "id": "lfw-gS03l5F4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_smote_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
      ],
      "metadata": {
        "id": "_xi0djrpmvXx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_xi0djrpmvXx"
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "ZQZEJVgamvXy"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZQZEJVgamvXy"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "rf_smote = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(rf_smote), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "1ZZwJsTIl5On"
      },
      "id": "1ZZwJsTIl5On",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_smote_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OnT092gZl5VX"
      },
      "id": "OnT092gZl5VX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_smote_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall Curve with Class weights\")"
      ],
      "metadata": {
        "id": "6aG6Xr2mmTpl"
      },
      "id": "6aG6Xr2mmTpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## XGBoost - imbalanced dataset\n",
        "def xgboost_search(X, y, search_verbose=1):\n",
        "    params = {\n",
        "    \"gamma\":[0.5, 1, 1.5, 2, 5],\n",
        "    \"max_depth\":[3,4,5,6],\n",
        "    \"min_child_weight\": [100],\n",
        "    \"subsample\": [0.6, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "    \"learning_rate\": [0.1, 0.01, 0.001]\n",
        "    }\n",
        "    xgb = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"auc\", use_label_encoder=False)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1234)\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=xgb, param_grid=params, scoring=\"roc_auc\", n_jobs=1, cv=skf.split(X,y), verbose=search_verbose)\n",
        "\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "    print(\"Best estimator: \")\n",
        "    print(grid_search.best_estimator_)\n",
        "    print(\"Parameters: \", grid_search.best_params_)\n",
        "    print(\"Highest AUC: %.2f\" % grid_search.best_score_)\n",
        "\n",
        "    return grid_search.best_params_"
      ],
      "metadata": {
        "id": "2SqG24ZEmTvk"
      },
      "id": "2SqG24ZEmTvk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "rows = random.sample(np.arange(0,len(X_train.index)).tolist(), 5000)"
      ],
      "metadata": {
        "id": "9XxVgznNmYXH"
      },
      "id": "9XxVgznNmYXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest - imbalanced dataset #\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=False)"
      ],
      "metadata": {
        "id": "KKSBXJwJ44p9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KKSBXJwJ44p9"
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(random_state=13)"
      ],
      "metadata": {
        "id": "l5OVvMAc44p-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "l5OVvMAc44p-"
    },
    {
      "cell_type": "code",
      "source": [
        "score = cross_val_score(xgb, X_train, y_train, cv=kf)\n",
        "print(\"Cross Validation Recall scores are: {}\".format(score))\n",
        "print(\"Average Cross Validation Recall score: {}\".format(score.mean()))"
      ],
      "metadata": {
        "id": "JzK1XcI444p-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JzK1XcI444p-"
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.fit(X_train, y_train.ravel())"
      ],
      "metadata": {
        "id": "scTJ4WvEAG3-"
      },
      "id": "scTJ4WvEAG3-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSnmdgwoALqU"
      },
      "id": "LSnmdgwoALqU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGLzj4ApAL0b"
      },
      "id": "cGLzj4ApAL0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDZ0ccV6AL7g"
      },
      "id": "UDZ0ccV6AL7g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params={\n",
        " \"learning_rate\"    : [0.05,0.1, 0.3] ,\n",
        " \"max_depth\"        : [ 12, 15]\n",
        "\n",
        "}\n",
        "grid_xgb= GridSearchCV(estimator = xgb,param_grid=params,scoring='f1',n_jobs=-1,cv=kf)\n",
        "\n",
        "# grid_rf = GridSearchCV(rf, param_grid=params, cv=kf,\n",
        "#                           scoring='recall').fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "U4d0ETkw44p-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "U4d0ETkw44p-"
    },
    {
      "cell_type": "code",
      "source": [
        "grid_xgb.fit(X_train,y_train.ravel())"
      ],
      "metadata": {
        "id": "qbQCCyO67ldG"
      },
      "id": "qbQCCyO67ldG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best parameters:', grid_xgb.best_params_)\n",
        "print('Best score:', grid_xgb.best_score_)"
      ],
      "metadata": {
        "id": "WCvQ73X38LK5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WCvQ73X38LK5"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid_xgb.predict(X_test)"
      ],
      "metadata": {
        "id": "nf0gcgOb8LK6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nf0gcgOb8LK6"
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "yjCSNUM18LK6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yjCSNUM18LK6"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "xgb_matrix = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "class_names=['Non-Fraud', 'Fraud'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "sns.heatmap(pd.DataFrame(xgb_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = [0.5, 1.5]\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)"
      ],
      "metadata": {
        "id": "WgSoTl8c8LK6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WgSoTl8c8LK6"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Get predicted probabilities from the best Logistic Regression model\n",
        "y_proba = grid_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Calculate the AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=\"Random Forest, AUC={:.3f}\".format(auc))\n",
        "plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n",
        "\n",
        "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "# Display AUC score at the bottom left of the curve\n",
        "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size': 13}, loc='lower right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wHugowcP8LK6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wHugowcP8LK6"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "display = PrecisionRecallDisplay.from_estimator(\n",
        "    grid_rf, X_test, y_test, name=\"Average precision\")\n",
        "_ = display.ax_.set_title(\"Precision-Recall curve with Class weights\")"
      ],
      "metadata": {
        "id": "XUzYztM68LK6"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XUzYztM68LK6"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDLDHV9C44QK"
      },
      "id": "hDLDHV9C44QK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChKVLUEy44UH"
      },
      "id": "ChKVLUEy44UH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#innput-output split\n",
        "X = df_num.drop(['isfraud'],axis=1)\n",
        "y = df_num.isfraud"
      ],
      "metadata": {
        "id": "4t4nsF50QEPR"
      },
      "id": "4t4nsF50QEPR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYMe0SkcrJvP"
      },
      "outputs": [],
      "source": [
        "#train-test split using stratified K fold\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "skf.get_n_splits(X,y)\n",
        "\n",
        "for train_index, test_index in skf.split(X,y):\n",
        "  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "y_train.value_counts()"
      ],
      "id": "DYMe0SkcrJvP"
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(random_state=42, solver = 'liblinear')\n",
        "model = lr.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "print('y_train_pred: ',y_train_pred)\n",
        "y_test_pred = model.predict(X_test)\n",
        "print('y_test_pred: ', y_test_pred)"
      ],
      "metadata": {
        "id": "ZVs6lAu3RO5H"
      },
      "id": "ZVs6lAu3RO5H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the model\n",
        "model_name = 'Logistic Regression - without balancing'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(y_test,y_test_pred)\n",
        "f_score = f1_score(y_test, y_test_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = metrics.recall_score(y_test,y_test_pred)\n",
        "#creating a dataframe to compare the performance of different models\n",
        "model_eval_data = [[model_name, train_score, test_score, acc_score, f_score, precision, recall]]\n",
        "evaluate_df = pd.DataFrame(model_eval_data, columns=['Model Name', 'Training Score', 'Testing Score', 'Accuracy',\n",
        "                                          'F1 Score', 'Precision', 'Recall'])\n",
        "evaluate_df"
      ],
      "metadata": {
        "id": "DTb_UUX6RVDb"
      },
      "id": "DTb_UUX6RVDb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming X_test and y_test variables\n",
        "test_input = X_test.copy()\n",
        "test_output = y_test.copy()\n",
        "\n",
        "test_output.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "yFK8AiglSopL"
      },
      "id": "yFK8AiglSopL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing all fraud transactions\n",
        "fraud_trans = df_num[df_num['isfraud'] == 1]\n",
        "non_fraud_trans = df_num[df_num['isfraud'] == 0]\n",
        "\n",
        "print('fraud data shape: ', fraud_trans.shape)\n",
        "print('non fraud data shape: ', non_fraud_trans.shape)\n",
        "\n",
        "#printing fraud data percentage\n",
        "print('Fraud Data percentage: ', 100*(len(fraud_trans)/len(non_fraud_trans)))"
      ],
      "metadata": {
        "id": "80U45oSSSp85"
      },
      "id": "80U45oSSSp85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler()\n",
        "X_rus, y_rus = rus.fit_resample(X_train,y_train)\n",
        "\n",
        "y_rus.value_counts()"
      ],
      "metadata": {
        "id": "ZAphpYs3SqEy"
      },
      "id": "ZAphpYs3SqEy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test Split\n",
        "X_train_undersampled, X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_rus, y_rus, test_size=0.3, random_state=42, stratify=y_rus)\n",
        "y_train_undersampled.value_counts()"
      ],
      "metadata": {
        "id": "tDfuYyjRS1g5"
      },
      "id": "tDfuYyjRS1g5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a baseline linear model\n",
        "lr = LogisticRegression(random_state=42, max_iter = 1000)\n",
        "#model\n",
        "model = lr.fit(X_train_undersampled, y_train_undersampled)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_train_pred"
      ],
      "metadata": {
        "id": "iLGsuMgcU_NZ"
      },
      "id": "iLGsuMgcU_NZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting on imbalanced test data\n",
        "test_pred=model.predict(test_input)"
      ],
      "metadata": {
        "id": "-jn0GCiSVUen"
      },
      "id": "-jn0GCiSVUen",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the classification report of the model\n",
        "print(classification_report(test_output,test_pred))"
      ],
      "metadata": {
        "id": "dKH7g87lVVMg"
      },
      "id": "dKH7g87lVVMg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the model\n",
        "model_name = 'Logistic Regression - Random Under Sampling'\n",
        "train_score = model.score(X_train_undersampled,y_train_undersampled)\n",
        "test_score = model.score(X_test_undersampled,y_test_undersampled)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred)\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "metadata": {
        "id": "kNfqPzQzTYI7"
      },
      "id": "kNfqPzQzTYI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#oversampling with imblearn\n",
        "ros = RandomOverSampler()\n",
        "X_ros, y_ros = ros.fit_resample(X_train,y_train)\n",
        "\n",
        "y_ros.value_counts()"
      ],
      "metadata": {
        "id": "yddVgZqVTYNn"
      },
      "id": "yddVgZqVTYNn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "skf.get_n_splits(X, y)"
      ],
      "metadata": {
        "id": "2McRcBVfMFCj"
      },
      "id": "2McRcBVfMFCj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_df = pd.DataFrame(columns=['Model Name', 'Training Score', 'Testing Score', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])"
      ],
      "metadata": {
        "id": "rE3pQiKiMHcH"
      },
      "id": "rE3pQiKiMHcH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Initialize and train a Logistic Regression model\n",
        "    lr = LogisticRegression(random_state=42, max_iter = 1000, solver = 'liblinear')\n",
        "    model = lr.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    model_name = 'Logistic Regression - without balancing'\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    acc_score = accuracy_score(y_test, y_test_pred)\n",
        "    f_score = f1_score(y_test, y_test_pred, average='weighted')\n",
        "    precision = precision_score(y_test, y_test_pred)\n",
        "    recall = recall_score(y_test, y_test_pred)\n",
        "\n",
        "    # Store evaluation results in the dataframe\n",
        "    model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "    model_eval_dict = {evaluate_df.columns[i]: model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "    evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)"
      ],
      "metadata": {
        "id": "ZE3a20QCMHj4"
      },
      "id": "ZE3a20QCMHj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler()\n",
        "X_rus, y_rus = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "SQ7ReShxMHqy"
      },
      "id": "SQ7ReShxMHqy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test Split\n",
        "X_train_undersampled, X_test_undersampled, y_train_undersampled, y_test_undersampled = train_test_split(X_rus, y_rus, test_size=0.3, random_state=42, stratify=y_rus)\n",
        "\n",
        "# Create a new Logistic Regression model on the under-sampled data\n",
        "lr_undersampled = LogisticRegression(random_state=42)\n",
        "model_undersampled = lr_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
        "\n",
        "# Predictions on under-sampled test data\n",
        "test_pred_undersampled = model_undersampled.predict(X_test_undersampled)\n",
        "\n",
        "# Print classification report for the under-sampled model\n",
        "print(classification_report(y_test_undersampled, test_pred_undersampled))"
      ],
      "metadata": {
        "id": "8iz99plEOvop"
      },
      "id": "8iz99plEOvop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Logistic Regression - Random Under Sampling'\n",
        "train_score_undersampled = model_undersampled.score(X_train_undersampled, y_train_undersampled)\n",
        "test_score_undersampled = model_undersampled.score(X_test_undersampled, y_test_undersampled)\n",
        "\n",
        "acc_score_undersampled = accuracy_score(y_test_undersampled, test_pred_undersampled)\n",
        "f_score_undersampled = f1_score(y_test_undersampled, test_pred_undersampled, average='weighted')\n",
        "precision_undersampled = precision_score(y_test_undersampled, test_pred_undersampled)\n",
        "recall_undersampled = recall_score(y_test_undersampled, test_pred_undersampled)\n"
      ],
      "metadata": {
        "id": "RZHsfYwhOvxQ"
      },
      "id": "RZHsfYwhOvxQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add under-sampled model evaluation results to the dataframe\n",
        "model_eval_data_undersampled = [model_name, train_score_undersampled, test_score_undersampled, acc_score_undersampled, f_score_undersampled, precision_undersampled, recall_undersampled]\n",
        "model_eval_dict_undersampled = {evaluate_df.columns[i]: model_eval_data_undersampled[i] for i in range(len(model_eval_data_undersampled))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict_undersampled, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "2TRbGwsnO8WJ"
      },
      "id": "2TRbGwsnO8WJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the evaluation dataframe\n",
        "evaluate_df"
      ],
      "metadata": {
        "id": "7hg6-PMlO8bV"
      },
      "id": "7hg6-PMlO8bV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVIYeQLGO8iH"
      },
      "id": "aVIYeQLGO8iH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oa3I9CPWmA1"
      },
      "outputs": [],
      "source": [
        "x = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)"
      ],
      "id": "2Oa3I9CPWmA1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNP0r3DTr0gb"
      },
      "outputs": [],
      "source": [
        "# # Scaling the values\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)"
      ],
      "id": "aNP0r3DTr0gb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3B2GKVKLJp"
      },
      "outputs": [],
      "source": [
        "#### Logistic Regression #####"
      ],
      "id": "cs3B2GKVKLJp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-PjUDxcSGiM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "skf.get_n_splits(X,y)"
      ],
      "id": "A-PjUDxcSGiM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxMpaEsDTk3P"
      },
      "outputs": [],
      "source": [
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "y_train.value_counts()"
      ],
      "id": "wxMpaEsDTk3P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnOPGab0-qhG"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(random_state=42)\n",
        "model = lr.fit(X_train, y_train.values.ravel())  # Use y_train.values.ravel()\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "id": "WnOPGab0-qhG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ6mXfbr-3ri"
      },
      "outputs": [],
      "source": [
        "#evaluating the model\n",
        "model_name = 'Logistic Regression - without balancing'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(y_test,y_test_pred)\n",
        "f_score = f1_score(y_test, y_test_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = metrics.recall_score(y_test,y_test_pred)\n",
        "#creating a dataframe to compare the performance of different models\n",
        "model_eval_data = [[model_name, train_score, test_score, acc_score, f_score, precision, recall]]\n",
        "evaluate_df = pd.DataFrame(model_eval_data, columns=['Model Name', 'Training Score', 'Testing Score', 'Accuracy',\n",
        "                                          'F1 Score', 'Precision', 'Recall'])\n",
        "evaluate_df"
      ],
      "id": "hJ6mXfbr-3ri"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fUvsUmmaNym"
      },
      "outputs": [],
      "source": [
        "# renaming X_test and y_test variables\n",
        "test_input = X_test.copy()\n",
        "test_output = y_test.copy()\n",
        "test_output.value_counts(normalize = True)"
      ],
      "id": "0fUvsUmmaNym"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf6m5cdMXQUZ"
      },
      "outputs": [],
      "source": [
        "#storing all fraud transactions\n",
        "fraud_trans = df_num[df_num['isfraud'] == 1]\n",
        "non_fraud_trans = df_num[df_num['isfraud'] == 0]\n",
        "\n",
        "print('fraud data shape: ', fraud_trans.shape)\n",
        "print('non fraud data shape: ', non_fraud_trans.shape)\n",
        "\n",
        "#printing fraud data percentage\n",
        "print('Fraud Data percentage: ', 100*(len(fraud_trans)/len(non_fraud_trans)))"
      ],
      "id": "xf6m5cdMXQUZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXQtwJWUgqbx"
      },
      "outputs": [],
      "source": [
        "#modules for model building\n",
        "#algorithms for sampling\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "id": "wXQtwJWUgqbx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjcRq05OhEC-"
      },
      "outputs": [],
      "source": [
        "# Random UnderSampling\n",
        "rus = RandomUnderSampler()\n",
        "X_rus, y_rus = rus.fit_resample(X_train,y_train)\n",
        "y_rus.value_counts()"
      ],
      "id": "BjcRq05OhEC-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYTtIDunhsBw"
      },
      "outputs": [],
      "source": [
        "# Train - test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size = 0.3, random_state = 42, stratify = y_rus)\n",
        "y_train.value_counts()"
      ],
      "id": "WYTtIDunhsBw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95NulurOX2ph"
      },
      "outputs": [],
      "source": [
        "# Creating a baseline linear model\n",
        "lr = LogisticRegression(random_state=42)\n",
        "# Model\n",
        "model = lr.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_train_pred"
      ],
      "id": "95NulurOX2ph"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ_vTWFAX5Cn"
      },
      "outputs": [],
      "source": [
        "#predicting on imbalanced test data\n",
        "test_pred=model.predict(test_input)\n",
        "test_pred"
      ],
      "id": "yJ_vTWFAX5Cn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ef18emFX7vb"
      },
      "outputs": [],
      "source": [
        "#printing the classification report of the model\n",
        "print(classification_report(test_output,test_pred))"
      ],
      "id": "6ef18emFX7vb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rBRIoeoYBAA"
      },
      "outputs": [],
      "source": [
        "#evaluating the model\n",
        "model_name = 'Logistic Regression - Random Under Sampling'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred, average='weighted')\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "2rBRIoeoYBAA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Zi0qtaj3HM"
      },
      "outputs": [],
      "source": [
        "ros = RandomOverSampler()\n",
        "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
        "y_ros.value_counts()"
      ],
      "id": "K_Zi0qtaj3HM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CptzPusekxJq"
      },
      "outputs": [],
      "source": [
        "X_train, X_test,y_train,y_test = train_test_split(X_ros, y_ros, test_size = 0.3, stratify = y_ros, random_state = 42)\n",
        "y_train.value_counts()"
      ],
      "id": "CptzPusekxJq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAT_ZeKYlIlI"
      },
      "outputs": [],
      "source": [
        "## Oversampling - SMOTE method\n",
        "smote = SMOTE()\n",
        "X_sm,y_sm = smote.fit_resample(X_train.astype('float'),y_train)\n",
        "y_sm.value_counts()"
      ],
      "id": "eAT_ZeKYlIlI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lNqRI-63Djs"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_sm,y_sm,test_size = 0.3, random_state = 42, stratify = y_sm)\n",
        "y_train.value_counts()"
      ],
      "id": "6lNqRI-63Djs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZVbUvhZY0Cs"
      },
      "outputs": [],
      "source": [
        "#implementing logistic regression\n",
        "lr = LogisticRegression(random_state=42)\n",
        "#creating model\n",
        "model = lr.fit(X_train, y_train)\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_train_pred"
      ],
      "id": "EZVbUvhZY0Cs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zz-4LNHY1tz"
      },
      "outputs": [],
      "source": [
        "test_pred = model.predict(test_input)\n",
        "test_pred"
      ],
      "id": "_Zz-4LNHY1tz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT8P5d6mY3tY"
      },
      "outputs": [],
      "source": [
        "#printing classification report\n",
        "print(classification_report(test_output, test_pred))"
      ],
      "id": "bT8P5d6mY3tY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGs3pKNVY7Z6"
      },
      "outputs": [],
      "source": [
        "#evaluating the model\n",
        "model_name = 'Logistic Regression - SMOTE'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred, average='weighted')\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "tGs3pKNVY7Z6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifHrEyBKZ_9B"
      },
      "outputs": [],
      "source": [
        "# Decision tree without sampling\n",
        "#train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
        "\n",
        "dtree = DecisionTreeClassifier(max_depth=10)\n",
        "\n",
        "model = dtree.fit(X_train,y_train)\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "#evaluating the model\n",
        "model_name = 'Decision Tree - without balancing'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(y_test,y_test_pred)\n",
        "f_score = f1_score(y_test, y_test_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = metrics.recall_score(y_test,y_test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "ifHrEyBKZ_9B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuXdhnrhvjYp"
      },
      "outputs": [],
      "source": [
        "# Decision Tree Undersampling\n",
        "#train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rus,y_rus, test_size=0.3, random_state=42)\n",
        "\n",
        "dtree = DecisionTreeClassifier(max_depth=10)\n",
        "\n",
        "model = dtree.fit(X_train,y_train)\n",
        "\n",
        "test_pred = model.predict(test_input)\n",
        "\n",
        "print(classification_report(test_output, test_pred))\n",
        "\n",
        "#evaluating the model\n",
        "model_name = 'Decision Tree - Random Under Sampling'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred, average='weighted')\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "QuXdhnrhvjYp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEtVlgI6wSwS"
      },
      "outputs": [],
      "source": [
        "#Decision tree SMOTE\n",
        "#train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sm,y_sm, test_size=0.3, random_state=42)\n",
        "\n",
        "dtree = DecisionTreeClassifier(max_depth=10)\n",
        "\n",
        "model = dtree.fit(X_train,y_train)\n",
        "\n",
        "test_pred = model.predict(test_input)\n",
        "\n",
        "print(classification_report(test_output, test_pred))\n",
        "\n",
        "#evaluating the model\n",
        "model_name = 'Decision Tree - SMOTE'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred, average='weighted')\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "GEtVlgI6wSwS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_HnpnkBbbjQ"
      },
      "outputs": [],
      "source": [
        "#Random Forest without sampling\n",
        "#train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
        "\n",
        "model = rf.fit(X_train,y_train)\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "#evaluating the model\n",
        "model_name = 'Random Forest - without balancing'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(y_test,y_test_pred)\n",
        "f_score = f1_score(y_test, y_test_pred, average='weighted')\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = metrics.recall_score(y_test,y_test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "q_HnpnkBbbjQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5bIdwBuaZpY"
      },
      "outputs": [],
      "source": [
        "#train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_rus,y_rus, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
        "\n",
        "model = rf.fit(X_train,y_train)\n",
        "\n",
        "test_pred = model.predict(test_input)\n",
        "\n",
        "print(classification_report(test_output, test_pred))\n",
        "\n",
        "#evaluating the model\n",
        "model_name = 'Random Forest - Random Under Sampling'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred, average='weighted')\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "E5bIdwBuaZpY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ-aEhbPaiHZ"
      },
      "outputs": [],
      "source": [
        "#train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sm,y_sm, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
        "\n",
        "model = rf.fit(X_train,y_train)\n",
        "\n",
        "test_pred = model.predict(test_input)\n",
        "\n",
        "print(classification_report(test_output, test_pred))\n",
        "\n",
        "#evaluating the model\n",
        "model_name = 'Random Forest - SMOTE'\n",
        "train_score = model.score(X_train,y_train)\n",
        "test_score = model.score(X_test,y_test)\n",
        "\n",
        "acc_score = accuracy_score(test_output,test_pred)\n",
        "f_score = f1_score(test_output, test_pred, average='weighted')\n",
        "precision = precision_score(test_output, test_pred)\n",
        "recall = metrics.recall_score(test_output,test_pred)\n",
        "#adding claculations to dataframe\n",
        "model_eval_data = [model_name, train_score, test_score, acc_score, f_score, precision, recall]\n",
        "model_eval_dict = {evaluate_df.columns[i]:model_eval_data[i] for i in range(len(model_eval_data))}\n",
        "evaluate_df = evaluate_df.append(model_eval_dict, ignore_index=True)\n",
        "\n",
        "evaluate_df"
      ],
      "id": "mZ-aEhbPaiHZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heS9fb5C3b_-"
      },
      "outputs": [],
      "source": [
        "sm = SMOTE()\n",
        "X_train_new, y_train_new = sm.fit_resample(X_train, y_train.ravel())\n",
        "\n",
        "# to demonstrate the effect of SMOTE over imbalanced datasets\n",
        "fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize =(15, 5))\n",
        "ax1.set_title('Before SMOTE')\n",
        "pd.Series(y_train).value_counts().plot.bar(ax=ax1)\n",
        "\n",
        "ax2.set_title('After SMOTE')\n",
        "pd.Series(y_train_new).value_counts().plot.bar(ax=ax2)\n",
        "\n",
        "plt.show()"
      ],
      "id": "heS9fb5C3b_-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h93z9Cvrq2Tv"
      },
      "outputs": [],
      "source": [],
      "id": "h93z9Cvrq2Tv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6j2nro-5Def"
      },
      "outputs": [],
      "source": [],
      "id": "O6j2nro-5Def"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSRX3svTrGaV"
      },
      "outputs": [],
      "source": [],
      "id": "sSRX3svTrGaV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA0KVrAaanqR"
      },
      "outputs": [],
      "source": [],
      "id": "CA0KVrAaanqR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7s1CcqAeuru"
      },
      "outputs": [],
      "source": [],
      "id": "z7s1CcqAeuru"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}